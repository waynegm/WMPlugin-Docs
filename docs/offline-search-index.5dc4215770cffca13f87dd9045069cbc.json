[{"body":"Script: Miscellaneous/ex_add_noise.py\nDescription This Python External Attribute script adds gaussian distributed noise to an input signal.\nExamples This example shows an input signal with different levels of added noise.\nInput with varying levels of added noise\nInput Parameters ex_add_noise.py input parameters\nNAME DESCRIPTION S/N Ratio Desired signal to noise ratio. ","categories":["docs"],"description":"add gaussian distributed noise to an input","excerpt":"add gaussian distributed noise to an input","ref":"/WMPlugin-Docs/docs/externalattributes/addnoise/","tags":["ExternalAttrib","modelling"],"title":"Add Noise"},{"body":"This attribute plugin for the open source seismic interpretation platform [opendtect] calculates 6 attributes based on different parameterizations of reflection intercept and gradient.\nDescription The intercept-gradient crossplot is widely used for amplitude-variation-with-offset (AVO) analysis in hydrocarbon exploration. The intercept is the zero offset or normal incidence reflection amplitude/coefficient of an event while the gradient is the change in reflection amplitude/coefficient with offset or incidence angle. Some authors refer to intercept as A or P and gradient as B or G. Modelling studies show that changes in subsurface rock properties such as lithology, porosity and pore fluid content result in systematic changes in intercept-gradient space. The following figure, adapted from the very informative paper on AVO by Foster etal (2010), illustrates the ideal intercept-gradient crossplot response for a clastic sequence to changes in porosity and pore fluid. The Fluid Line or Background Trend is where reflections from shales and some wet sands that have little contrast in Vp/Vs plot.\nIdeal AVO crossplot for a clastic sequence\nAnother popular classification of AVO effects on the intercept-gradient crossplot is the 3 class AVO scheme of Rutherford and Williams (1989) based on acoustic impedance contrasts with the addition of a 4th class for high porosity gas sands introduced by Castagna and Swan (1997). The position of these 4 AVO classes on the intercept-gradient crossplot is shown in the following figure.\nAVO classes\nThis plugin calculates 6 attributes which are essentially just coordinate transformations of the intercept and gradient that aim to highlight changes of a particular rock property based on the expected behaviour of clastic reservoir sequences.\nATTRIBUTE DESCRIPTION Fluid Factor Fluid Factor is a reprojection of intercept/gradient coordinates that highlights deviation from the Fluid Line. As the name implies this attribute highlights changes related to fluid compressibility. The Fluid Factor was introduced by Smith and Gidlow (1987). Lithology Factor Lithology Factor is the companion coordinate reprojection to Fluid Factor. It highlights changes parallel to the Fluid Line. Porosity Factor Porosity Factor is just Lithology Factor with the values above the fluid line reversed so the attribute magnitude always increases in the same sense as the effect of increasing porosity, i.e. the attribute magnitude increases to the upper left below the Fluid Line and to the lower right above the Fluid Line. Crossplot Angle Crossplot Angle is the angle between an intercept-gradient point and the Fluid Line. It runs from 0 to 180 below the Fluid Line measured counter-clockwise from the upper left projection of the Fluid Line. It runs from 0 to -180 above the Fluid Line measured counter-clockwise from the lower right projection of the Fluid Line. The Crossplot Angle and Deviation can be interpreted as the polar coordinates of a data point in the intercept-gradient crossplot space. Crossplot Deviation Crossplot Deviation is the distance from the crossplot origin. This attribute is most useful as a transparency mask to remove data close to the centre of the crossplot. AVO Class AVO Class classifies intercept-gradient points according to the 4 class AVO scheme of Rutherford and Williams (1989) and Castagna and Swan (1997). Points below the Fluid Line have a positive class number and those above have a negative class number. Input Parameters These attributes have 4 required parameters and 4 extra parameters that may be required depending on the attribute being calculated:\nNAME DESCRIPTION Intercept Volume The attribute volume to use as the zero offset or normal incidence reflection amplitude coefficient. If no intercept volume is available a near angle or offset stack can be used as an alternative. Gradient Volume The attribute volume to use as the change in reflection amplitude/coefficient with offset at normal incidence. If no gradient volume is available, the difference between amplitudes on far and near angle or offset stacks can be used as an alternative. Output The attribute to calculate. There is a choice of Fluid Factor, Lithology Factor, Porosity Factor, Crossplot Angle, Crossplot Deviation or AVO Class. Crossplot Slope The slope of the fluid line interpreted on the intercept-gradient crossplot. This can be read from the properties dialog of the crossplot tool. Intercept Standard Deviation (Optional) Standard deviation of the intercept volume. Only required for the Crossplot Angle and Crossplot Deviation attributes. This can be read from the 1D histogram tool in the crossplot table. Gradient Standard Deviation (Optional) Standard deviation of the gradient volume. Only required for the Crossplot Angle and Crossplot Deviation attributes. This can be read from the 1D histogram tool in the crossplot table. Correlation Coefficient (Optional) Correlation coefficient between the intercept and gradient volumes. Only required for the Crossplot Deviation attribute. This can be read from the properties dialog of the crossplot tool. Class 2 Intercept Offset Half width in intercept coordinates of the Class 2 region on the AVO Class crossplot. Only required for the AVO Class attribute.’]] AVO Attribute Plugin input parameters\n","categories":["docs"],"description":"6 attributes based on different parameterizations of reflection intercept and gradient","excerpt":"6 attributes based on different parameterizations of reflection …","ref":"/WMPlugin-Docs/docs/plugins/avoattrib/","tags":["AVOAttrib","avo"],"title":"AVOAttrib"},{"body":"As of OpendTect 6.4.5 this suite of plugins can be installed using the OpendTect Installation Manager. Just activate the WMPlugins option in the installer.\nOpendTect Installation Manager\nThis will automatically install the latest available version of the plugin for your OpendTect installation into the OpendTect installation folder.\nCompiled versions of the plugins for Linux x86_64 and Windows x86_64 are also available for download from the GitHub project release page.\nNote that the plugin version installed should match the OpendTect version. For example the 6.4 series of plugin binaries should work with all OpendTect 6.4 releases but won’t work with OpendTect 6.0, 6.2 or 6.6.\nThe following instructions are only needed for a manual intallation.\nLinux Sitewide Installation To install the plugins into the OpendTect program folder (eg __/opt/seismic/OpendTect/6.4.0/ __):\nCopy the contents of the bin/lux64/Release/ folder in the tgz file to /opt/seismic/OpendTect/6.4.0/bin/lux64/Release/;\nCopy the contents of the plugins/lux64/ folder in the tgz file to /opt/Seismic/OpendTect/6.4.0/plugins/lux64/; and\nRestart OpendTect.\nPer-user Installation On Linux it is also possible to install the plugin files in a users .od folder. Note that the OpendTect-6.4.0-plugins won’t work in OpendTect 6.2.0 and the OpendTect-6.2.0-plugins won’t work in OpendTect 6.4.0. See the [faq] for a workaround if you want a per-user installation and want to run multiple versions of OpendTect.\nCopy the contents of the bin/lux64/Release/ folder in the tgz file to the users .od/bin/lux64/Release/ folder;\nCopy the contents of the plugins/lux64/ folder in the tgz file to the users .od/plugins/lux64/ folder; and\nRestart OpendTect.\nWindows Sitewide Installation To install the plugins into the OpendTect program folder (eg __c:\\Program Files\\Opendtect\\6.4.0 __):\nCopy the contents of the __bin\\win64\\Release\\ __ folder in the zip file to __c:\\Program Files\\Opendtect\\6.4.0\\bin\\win64\\Release\\ __;\nCopy the contents of the __plugins\\win64\\ __ folder in the zip file to __c:\\Program Files\\Opendtect\\6.4.0\\plugins\\win64\\ __; and\nRestart OpendTect.\nPer-user Installation On Windows it is also possible to install the plugin files in a users .od folder. Note that the OpendTect-6.4.0-plugins won’t work in OpendTect 6.2 and the OpendTect-6.2-plugins won’t work in OpendTect 6.4. See the [faq] for a workaround if you want a per-user installation and want to run multiple versions of OpendTect.\nCopy the contents of the __bin\\win64\\Release\\ __ folder in the zip file to the users __C:\\Users%username%\\.od\\bin\\win64\\Release\\ __ folder;\nCopy the contents of the __plugins\\win64\\ __ folder in the zip file to the users __C:\\Users%username%\\.od\\plugins\\win64\\ __ folder; and\nRestart OpendTect.\n","categories":"","description":"binary installation","excerpt":"binary installation","ref":"/WMPlugin-Docs/docs/gettingstarted/installation/","tags":"","title":"Installation"},{"body":"Description These External Attribute scripts estimate AVO intercept and gradient based on Shuey’s 2 term approximation to the Zoeppritz equation. Scripts are provided for 3, 4 and 5 angle stacks.\nIntercept and Gradient from Angle Stacks Script: Miscellaneous/ex_angle_stacks_3_to_AVOIG.py Script: Miscellaneous/ex_angle_stacks_4_to_AVOIG.py Script: Miscellaneous/ex_angle_stacks_5_to_AVOIG.py\nTakes as input angle stacks and the corresponding angles and fits a least squares line to the \\(amplitude\\) and \\(sin^2(angle)\\) at each sample point. Output includes the intercept, gradient and the correlation coefficient of the line fit.\nInput Parameters ex_angle_stacks_4_to_AVOIG.py input parameters\nFor each input volume the corresponding incident angle must be provided.\n","categories":["docs"],"description":"estimate AVO intercept and gradient from angle stacks","excerpt":"estimate AVO intercept and gradient from angle stacks","ref":"/WMPlugin-Docs/docs/externalattributes/avo_ig/","tags":["ExternalAttrib","avo"],"title":"AVO Intercept and Gradient"},{"body":"This attribute plugin for the open source seismic interpretation platform [opendtect] calculates 6 attributes derived from AVO Polarization in the AVO Intercept-Gradient crossplot as described by Mahob and Castagna (2003).\nDescription The intercept-gradient crossplot is widely used for amplitude-variation-with-offset (AVO) analysis in hydrocarbon exploration. The intercept is the zero offset or normal incidence reflection amplitude/coefficient of an event while the gradient is the change in reflection amplitude/coefficient with offset or incidence angle. Some authors refer to intercept as A or P and gradient as B or G. Traditional methods of AVO interpretation focus on individual sample points in isolation essentially treating them as reflection coefficients. Keho etal (2001) observed that this approach ignores the seismic wavelet. Convolving a seismic reflection coefficient with a typical seismic wavelet produces a series of points spread across all 4 quadrants of the AVO crossplot. Further distortions are introduced by residual time shifts across offsets and NMO stretch. Keho etal (2001) proposed analysing the AVO crossplot for small time windows of 0.5-1 times the wavelet wavelength as hodograms using the polarization angle as a key measure. Mahob and Castagna (2003) subsequently extended the analysis with a number of other measures to describe the hodogram.\nExample AVO hodogram crossplot for a 36 millisec TWT window\nThis plugin calculates the 6 attributes (Background Polarization Angle, Event Polarization Angle, Polarization Angle Difference, Strength, Polarization Product and Quality) defined by Mahob and Castagna (2003). The parameters are estimated by eigendecomposition of the covariance matrix for the intercept-gradient crossplot.\nEXAMPLE ATTRIBUTE Background Polarization Angle Polarization Angle for all AVO crossplot points over a user specified time/depth window and volume of traces. The polarization angle is the orientation of the largest eigenvector relative to the positive intercept axis and varies from -90 to 90 degrees. Event Polarization Angle Polarization Angle for AVO crossplot points in the user specified event time/depth window. Recommend using 0.5-1 times the wavelength. The polarization angle is the orientation of the largest eigenvector relative to the positive intercept axis and varies from -90 to 90 degrees. Polarization Angle Difference The difference between the event and background polarization angles. StrengthThe Mahob and Castagna measure of the distance of the hodogram points from the origin within the event time/depth window. Polarization Product The product of the Strength and Polarization Angle Difference attributes. Quality This is the ratio of the eigenvalue difference to the eigenvalue sum. It is a measure of the linearity of the points in the intercept-gradient crossplot. It ranges from 0 to 1 with higher values indicating the analysis points have a more linear hodogram and more reliable results. Input Parameters These attributes have 3 required parameters and 3 extra parameters that may be required depending on the attribute being calculated:\nNAME DESCRIPTION Intercept The attribute volume to use as the zero offset or normal incidence reflection amplitude coefficient. If no intercept volume is available a near angle or offset stack can be used as an alternative. Gradient The attribute volume to use as the change in reflection amplitude/coefficient with offset at normal incidence. If no gradient volume is available, the difference between amplitudes on far and near angle or offset stacks can be used as an alternative. Output The attribute to calculate. There is a choice of Background Polarization Angle, Event Polarization Angle, Polarization Angle Difference, Strength, Polarization Product or Quality. Background time/depth gate (Optional) The time/depth gate used to estimate the Background Polarization Angle. Only required for the Background Polarization Angle, Polarization Angle Difference and Polarization Product attributes. Stepout (Optional) The extent of the trace volume used to estimate the Background Polarization Angle. Only required for the Background Polarization Angle, Polarization Angle Difference and Polarization Product attributes. Event time/depth gate (Optional) The time/depth gate used to estimate the Event Polarization Angle. Required for the Event Polarization, Polarization Angle Difference, Polarization Product and Quality attributes. AVO Polarization Attribute Plugin input parameters\n","categories":["docs"],"description":"6 attributes derived from AVO Polarization in the AVO Intercept-Gradient crossplot","excerpt":"6 attributes derived from AVO Polarization in the AVO …","ref":"/WMPlugin-Docs/docs/plugins/avopolarattrib/","tags":["AVOPolarAttrib","avo"],"title":"AVOPolarAttrib"},{"body":"Building from Source Linux These instructions are for Linux.\nDownload the source for the plugins for the appropriate version of OpendTect from Github\nLatest OpendTect v6.6 OpendTect v6.4 OpendTect v5 OpendTect v4 Use the OpendTect installation manager to install the OpendTect developer packages and install any other packages required for compiling and building code for your operating environment as per the OpendTect Programmer’s Manual\nStart OpendTect\nSelect the Utilities-Tools-Create Plugin Devel. Env. menu item to create a development work folder (eg /home/user/ODWork).\nUnzip the attribute source zip archive downloaded in step 1 in the development work folder. This will overwrite the CMakeLists.txt in the development work folder and add the plugin source folders to the plugin folder.\nOptionally edit CMakeCache.txt in the development work folder and change Debug to Release.\nOpen a terminal, cd to the development work folder and type:\ncmake . make This should create the binary files for each plugin, lib*.so and libui*.so, in the bin folder (eg in ODWork/bin/lux64/Release/) and four *.alo files for each plugin in the root of the development work folder. Windows These instructions are for Windows.\nDownload the source for the plugins for the appropriate version of OpendTect from Github\nLatest OpendTect v6.6 OpendTect v6.4 OpendTect v5 OpendTect v4 Use the OpendTect installation manager to install the OpendTect developer packages and install any other packages required for compiling and building code for your operating environment as per the OpendTect Programmer’s Manual\nStart OpendTect\nSelect the Utilities-Tools-Create Plugin Devel. Env. menu item to create a development work folder (eg c:\\Users\\user\\ODWork).\nUnzip the attribute source zip archive downloaded in step 1 in the development work folder. This will overwrite the CMakeLists.txt in the development work folder and add the plugin source folders to the plugin folder.\nFollow the instructions in the OpendTect Programmer’s Manual to configure and build the plugins.\nThis should create the binary files for each plugin in the bin folder (eg in ODWork\\bin\\win64\\Release).\nOn Windows you must use “Release” build plugins with the “Release” version of OpendTect.\n","categories":"","description":"building the plugins from the source code on Github","excerpt":"building the plugins from the source code on Github","ref":"/WMPlugin-Docs/docs/gettingstarted/building-from-source/","tags":"","title":"Building from Source"},{"body":"","categories":"","description":"installation instructions","excerpt":"installation instructions","ref":"/WMPlugin-Docs/docs/gettingstarted/","tags":"","title":"Getting Started"},{"body":"Description These Python External Attribute scripts implement various algorithms to estimate orientation, ie dip or dip azimuth.\nAll scripts will estimate at least the following attributes:\nOUTPUT DESCRIPTION Inline Dip Event dip observed on a crossline in microseconds per metre for time surveys and millimetres per metre for depth surveys. Output can be positive or negative with the convention that events dipping towards larger inline numbers producing positive dips. Crossline Dip Event dip observed on an inline in microseconds per metre for time surveys and millimetres per metre for depth surveys. Output can be positive or negative with the convention that events dipping towards larger crossline numbers producing positive dips. True Dip Event dip in microseconds per metre for time surveys and millimetres per metre for depth surveys. Output is always positive. Dip Azimuth Azimuth of the True Dip direction relative to the survey orientation. Output ranges from -180 to 180 degrees. Positive azimuth is defined from the inline in the direction of increasing crossline numbers. Azimuth = 0 indicates that the dip is dipping in the direction of increasing crossline numbers. Azimuth = 90 indicates that the dip is dipping in the direction of increasing inline numbers. Some scripts may offer additional outputs such as a measure of event coherency or planarity.\nAll of the scripts require the numba Python package.\nOrientation from gradients Script: ex_gradient_dip.py\nCalculates orientation from inline, crossline and Z gradients. No filtering is applied Unfiltered gradient dip - crossline dip on an inline\nOrientation from vector filtered gradients Script: ex_vf_gradient3_dip.py\nUses Kroon’s (2009) 3 point derivative filter to estimate data gradients. Next gradient normal unit vectors are determined and smoothed using a vector filter. NAME DESCRIPTION Output What to calculate - choice of inline dip, crossline dip, true dip or dip azimuth. Z window (+/-samples) Specifies the extent of the analysis cube in the Z direction. Number of Z samples in cube will be \\((2*Zwindow+1)\\). Stepout Specifies the inline and crossline extent of the analysis cube. Number of samples in each direction will be \\((2*Stepout+1)\\). Filter Choice of Mean Dip, L1 Vector Median or L2 Vector Median. The aperture of the vector filtering is \\((2*ZStepOut-1)\\) \\(Z\\) samples and \\((2*Stepout-1)\\) samples in the inline and crossline direction. For example for a 5x5x5 analysis cube \\((Zwindow=2, Stepout=2)\\) the gradients and associated normal unit vectors are generated on a 3x3x3 cube and vector filtered. The outer samples are only used in the gradient calculation.\nVector filtered gradient dip - crossline dip on an inline - 5x5x5 input\nOrientation by the gradient structure tensor Scripts: ex_gradient3_st_dip.py \u0026 ex_gradient5_st_dip.py\nUses either Kroon’s (2009) 3 point or the Farid and Simoncelli (2004) 5 point derivative filter to estimate data gradients which are then used to form the gradient structure tensor.\nNAME DESCRIPTION Output What to calculate - choice of inline dip, crossline dip, true dip or dip azimuth. Z window (+/-samples) Specifies the extent of the analysis cube in the Z direction. Number of Z samples in cube will be \\((2*Zwindow+1)\\). Stepout Specifies the inline and crossline extent of the analysis cube. Number of samples in each direction will be \\((2*Stepout+1)\\). For the ex_gradient3 script the structure tensor is formed from an aperture of \\((2*ZStepOut-1)\\) \\(Z\\) samples and \\((2*Stepout-1)\\) samples in the inline and crossline direction.\nFor the ex_gradient5 script the structure tensor is formed from an aperture of \\((2*ZStepOut-2)\\) \\(Z\\) samples and \\((2*Stepout-1)\\) samples in the inline and crossline direction.\nGradient3 structure tensor dip - crossline dip on an inline - 5x5x5 input\nOrientation from the 3D complex trace phase Script: ex_phase3_dip.py\nCalculates orientation from the 3D complex trace phase gradients as per Barnes (2007). Kroon’s (2009) 3 point derivative filter is used to compute gradients. NAME DESCRIPTION Output What to calculate - choice of inline dip, crossline dip, true dip or dip azimuth. Z window (+/-samples) Specifies the length \\((2*Zwindow+1)\\) of the time domain operator used to generate the complex analytic signal (recommend \u003e= 15) Band Specifies the proportion of the frequency band to include when generating the complex analytic signal (recommend 0.9). Unfiltered phase dip - crossline dip on an inline\nOrientation from vector filtered 3D complex trace phase Script: ex_vf_phase3_dip.py\nCalculate orientation unit normal vectors using the 3D complex trace phase gradient and apply a vector filter. Kroon’s (2009) 3 point filter used to compute gradients. NAME DESCRIPTION Output What to calculate - choice of inline dip, crossline dip, true dip or dip azimuth. Z window (+/-samples) Specifies the length \\((2*Zwindow+1)\\) of the time domain operator used to generate the complex analytic signal (recommend \u003e= 15) Stepout Specifies the inline and crossline extent of the analysis cube. Number of samples in each direction will be \\((2*Stepout+1)\\). Filter Choice of Mean Dip, L1 Vector Median or L2 Vector Median. Vector Filter ZStepOut Specifies the extent of the analysis cube for vector filtering in the Z direction. Number of Z samples in cube will be \\((2*ZStepOut+1)\\). Band Specifies the proportion of the frequency band to include when generating the complex analytic signal (recommend 0.9). The aperture of the vector filter is \\((2*ZStepOut+1)\\) \\(Z\\) samples and \\((2*Stepout-1)\\) samples in the inline and crossline direction.\nMean Vector Filtered phase dip - crossline dip on an inline - 3x3x3\nOrientation using the envelope weighted 3D complex trace phase structure tensor Script: ex_weighted_phase3_st_dip.py\nForms a structure tensor from the 3D complex trace phase gradients. Tensor elements are weighted by the trace envelope as per Luo etal (2006). Kroon’s (2009) 3 point filter is used to compute gradients. NAME DESCRIPTION Output What to calculate - choice of inline dip, crossline dip, true dip or dip azimuth. Z window (+/-samples) Specifies the length \\((2*Zwindow+1)\\) of the time domain operator used to generate the complex analytic signal (recommend \u003e= 15) Stepout Specifies the inline and crossline extent of the analysis cube. Number of samples in each direction will be \\((2*Stepout+1)\\). Tensor ZStepOut Specifies the extent of the analysis cube for vector filtering in the Z direction. Number of Z samples in cube will be \\((2*ZStepOut+1)\\). Band Specifies the proportion of the frequency band to include when generating the complex analytic signal (recommend 0.9). The aperture of the structure tensor is \\((2*ZStepOut+1)\\) \\(Z\\) samples and \\((2*Stepout-1)\\) samples in the inline and crossline direction.\nStructure tensor phase dip - crossline dip on an inline - 3x3x3 tensor input\n","categories":["docs"],"description":"various scripts to estimate orientation","excerpt":"various scripts to estimate orientation","ref":"/WMPlugin-Docs/docs/externalattributes/dipazimuth/","tags":["ExternalAttrib","dipaz"],"title":"Dip and Azimuth"},{"body":"This attribute plugin for the open source seismic interpretation platform [opendtect] Version 6.0.0 or later performs spectral (time-frequency) decomposition using empirical fourier decomposition.\nDescription This plugin can be used as an alternative to the OpendTect FFT spectral decomposition attribute.\nIt does spectral decomposition using Empirical Fourier Decomposition (Zhou etal(2019). The plugin provides 2 new attributes that generate the mode decomposition and a spectral decomposition from the mode decomposition. These provide a signal analysis akin to Empirical Mode Decomposition. These attributes can work quite well for a simple mixture of frequency chirps but appear to be less useful for typical seismic data.\nEmpirical Fourier Mode/Spectral Decomposition\nInput Parameters EFD Modes This attribute has 3 parameters: NAME DESCRIPTION Input Data The attribute volume to be analysed. Number of Modes The number of modes expected. Output Mode The mode to output. EFD Modes input parameter dialog\nThe “Display EFD Modes panel” button will generate the attribute for a trace in the input data volume.\nEFD Spectral Decomposition This attribute has 4 parameters: NAME DESCRIPTION Input Data The attribute volume to be analysed. Number of Modes The number of modes expected. Output Frequency (Hz) The frequency to output. Output Frequency Step (Hz) The frequency step for output. EFD Spectral Decomposition input parameter dialog\nThe “Display Time/Frequency panel” button will generate the attribute for a trace in the input data volume.\n","categories":["docs"],"description":"spectral (time-frequency) decomposition using empirical fourier decomposition","excerpt":"spectral (time-frequency) decomposition using empirical fourier …","ref":"/WMPlugin-Docs/docs/plugins/efdattrib/","tags":["time-frequency\"","spectral-decomposition"],"title":"EFDAttrib"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/docs/plugins/","tags":"","title":"Plugins"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/docs/externalattributes/","tags":"","title":"External Attributes"},{"body":"This attribute plugin for the open source seismic interpretation platform [opendtect] allows attributes to be developed in Python.\nDescription With this plugin it is possible to calculate single and multitrace attributes using a Python script. The plugin supports multi-trace multi-attribute input and multi-attribute output as well as parallel execution.\nNOTE: Releases prior to 6.0.2 allowed multi-trace multi-attribute input only if all the attributes were in the same multi-attribute volume. In releases from 6.0.2 onward this limitation has been removed.\nInstead of doing the attribute calculation within OpendTect this plugin starts up a Python interpreter and runs a user specified Python script. Trace data is read/written to/from the Python script using stdin/stdout pipes. As described below the Python script must import the extattrib.py module that handles the stdin/stdout data IO and presents the trace data as a numpy array. The script must implement a doCompute method and define a Python dictionary describing the User Interface. Details are described below. Tips and Tricks to assist can be found in various [../articles]. There are also the [wmscripts] for everything from dip estimation to filtering described in the [../external_attributes] section of this documentation which are distributed with the WMPlugins suite and can be found in the bin/python/wmpy folder of the OpendTect installation.\nInput Parameters This attribute has 3 required parameters and optional parameters determined by the JSON parameter string provided by the Python attribute script:\nNAME DESCRIPTION Interpreter The Python interpreter to be used to run the script, eg /usr/bin/python3. As of version 6.6.1 and 6.4.13 this field will default to the OpendTect Python settings External File The external application to be used for attribute calculation. As of version 6.6.1 and 6.4.13 this field will default to the OpendTect/bin/python/wmpy folder. External Attribute Plugin input parameters\nThe button beside the Interpreter entry will force an update of the Interpreter field to the current OpendTect Python setting. This can be useful if you use the Python Settings dialog to change the default interpreter and want to apply the change within the attribute definition.\nThe button beside the External File entry will force reloading the attribute script and rebuild the user interface. This can be useful if you are editing a script, save it and want to verify the changes.\nThe button beside the External File entry will open the system web browser with the url specified by the Help entry in the attribute scripts JSON parameter string. The button is not displayed if this entry is absent.\nPython Script Structure Every Python attribute script has 5 sections. As an example consider the ex_dip.py script which converts inline and crossline dip to true dip and dip azimuth. It is an example of multi attribute, single trace input and output. Some basic understanding of Python and Numpy is assumed.\nThe Imports This is where external modules/libraries required by the script are loaded. At a minimum the script must load:\nthe Python sys and os modules the Numpy module (the fundamental package for scientific computing with Python) the external attribute module (extattrib.py) Generally sys, os and Numpy will be part of the Python installation. The extattrib module is part of the [wmscripts] package and its location is unknown to the Python installation unless we help out. The sys.path.insert call on line 11 provides this help by extending the default search path for Python modules to include the parent folder of the folder containing the script. This reflects the folder structure of the [wmscripts] package, so if you develop scripts outside this structure then you will need to change line 11 appropriately to append the location of extattrib.py to the module search path.\nOf course if your script requires other Python modules (eg SciPy, Numba) then add the appropriate import statements in this section.\nThe Parameters The xa.params global variable must be assigned a JSON object string describing the input parameters for the script. This JSON string is used by the plugin to build an input dialog box. This attribute is very simple specifying just 2 input volumes and 2 output volumes and a url for documentation. The plugin dynamically builds the following input dialog for this script:\nA variety of other input elements can be specified to build more complex input dialogs. See JSON parameter string for full details or look at other scripts to see what is possible.\nThe Compute Loop Initialisation The doCompute function is where the attribute calculation occurs. The function is divided into 2 parts some initialisation and the “while True:” loop, discussed in the next section, where the calculations actually take place. Any code in this initialisation section will be executed just once when the attribute script is run and is a good place to calculate constants for use in the Compute Loop.\nThis particular script shows how information stored in the SeismicInfo Block can be used to calculate some constants purely as an example. This attribute is so simple that no initialisation is actually required.\nThe Compute Loop This is where the attribute calculation takes place. The xa.doInput() and xa.doOutput() function calls control the input and output of seismic trace data between the script and OpendTect. Generally these should be the first and last statements within the compute loop.\nWithin the compute loop, some information about the current trace data such as the number of samples and the inline and crossline location are provided in the TraceInfo Block. These can be accessed using constructs like xa.TI[’nrsamp’]. This information is not required for this particular script.\nThe global Numpy array xa.Input contains the input trace data. xa.Input[’name of input attribute’] returns a Numpy array with the trace data for the current compute location. The shape of this Numpy array depends on the traces stepouts required by the attribute. As this particular script uses just single trace input (inline and crossline step out of 0) the Numpy array has a shape of (1,1,xa.TI[’nrsamp’]). In the more general case of a multi-trace attribute the Numpy array shape would be (xa.SI[’nrinl’], xa.SI[’nrcrl’], xa.TI[’nrsamp’]) and the input trace at the current location would be at the centre of the array, ie at index [xa.SI[’nrinl’]//2, xa.SI[’nrcrl’]//2,…].\nAttribute ouput must be put into the xa.Output global Numpy array before the xa.doOutput() function call. Each element (eg xa.Output[’name of output attribute’]) of the output array must have a shape of (1,1,xa.TI[’nrsamp’])\nThe Postamble This section is just boilerplate code that appears in every attribute script which should never be changed.\nJSON Parameter String The Python script can specify a set of parameters as a JSON object string. As of Release 6.6.8 the format of the JSON object string has changed to better support more flexible UI’s. In particular the restriction on no whitespace in the keys and values in the JSON string have been removed. Scripts using the Legacy format (Select and Par_0 to Par_5) should continue to work. The following keywords are supported:\nJSON KEYWORD Input (depreciated) TYPE String' DESCRIPTION Specifies a label to appear beside the input attribute selection UI element. Superceded by the “Inputs” keyword but is supported for backward compatibility. EXAMPLE Input: Input Data JSON KEYWORD Inputs TYPE Array of Strings DESCRIPTION Each string is used as a label for an input attribute selection UI element. Currently limited to a maximum of 6 attribute inputs. EXAMPLE Inputs: [Input 1,Input 2,Input 3] JSON KEYWORD Output (optional) TYPE Array of Strings DESCRIPTION Each string specifies the name of an output attribute. If this keyword is not supplied a single output attribute is assumed. EXAMPLE Output: [Out 1,Out 2,Out 3] JSON KEYWORD ZSampMargin (optional) TYPE Object with a ‘Value’ (array of 2 numbers) and optional ‘Hidden’ (boolean) and ‘Symmetric’ (boolean) parameters. DESCRIPTION The 'Value' parameter is an array of 2 numbers specifying the desired minimum number of samples before and after the calculation point required for the calculation respectively. If not supplied only a single value will be provided when the attribute is computed on a timeslice or horizon. The optional ‘Hidden’ parameter is a boolean which if set to true makes the ZSampMargin parameter read only.The optional 'Symmetric' parameter is a boolean which if true causes only a single entry to be displayed in the UI.The optional 'Minimum' parameter is an array of 2 numbers specifying a minimum required window size EXAMPLE ZSampMargin: {Value: [-2,2]} ZSampMargin: {Value: [-2,2], Symmetric: True} JSON KEYWORD StepOut (optional) TYPE Object with a ‘Value’ (array of 2 numbers) and optional ‘Hidden’ (boolean) parameters. DESCRIPTION The ‘Value’ parameter is an array of 2 numbers specifying the inline and crossline stepout defining the block of traces to be used around the current calculation position. If not supplied only a single trace is provided. The optional ‘Hidden’ parameter is a boolean which if set to true makes the StepOut parameter read only. The optional ‘Minimum’ parameter is an array of 2 numbers specifying a minimum required stepout. EXAMPLE StepOut: {Value: [2,2]}StepOut: {Value: [2,2], Hidden: True} JSON KEYWORD UI Name String (Introduced in release 6.4.8 - optional) TYPE Object with a ‘Type’ (string) and ‘Value’ (string or number depending on the ‘Type’ key) parameters. DESCRIPTION In this new entry the main key is the name that will appear in the UI. Note that this can now contain whitespace. The 'Type' parameter describes the input field type. Currently Number, Select and File are supported. Number is for a numeric input box, equivalent to the legacy Par_0 to Par_5 entries but with the advantage that the number and order in the UI is more flexible. Select displays a combo-box, equivalent to the legacy Select entry but with the advantage that there is no restriction on the number and order in the UI. File is a file selection UI.The 'Value' sets the default value displayed in the UI. For Number types it is a number, for the Select and File types it is a string.For the File type the Value determines the location where the file selection dialog opens and if a '*' is present the file filter. EXAMPLE A File UI Field: {Type: File, Value: Seismics/*.wvlt} A Number UI Field: {Type: Number, Value: 20}A Select UI Field: {Type: Select, Options: [option 1,option 2,option 3],Value: option 2} JSON KEYWORD Help (optional) TYPE String DESCRIPTION URL pointing to documentation for the external attribute. Causes an icon help button to be displayed in the UI. EXAMPLE Help: http://waynegm.github.io/OpendTect-Plugin-Docs/External-Attributes/LPA-Attributes/ JSON KEYWORD Parallel (optional) TYPE Boolean DESCRIPTION Default is True which allows parallel execution. If set to False then calculations only use a single thread. EXAMPLE Parallel: False JSON KEYWORD Select (Legacy optional) TYPE Object with a ‘Name’ (string), ‘Values’ (array of strings) and ‘Select’ (number) parameters. DESCRIPTION Displays a list box labeled ‘Name’ with options specified in ‘Values’ and default selection being item number ‘Select’. EXAMPLE Select: {Name: Type, Values: [None, Median, Average], Selection: 0} JSON KEYWORD Par_0, Par_1, Par_2, Par_3, Par_4, Par_5 (Legacy all optional) TYPE Object with a ‘Name’ (string) and ‘Value’ (number) parameter. DESCRIPTION Displays an entry box labeled ‘Name’ with default value ‘Value’. EXAMPLE Par_0: {Name: First Parameter, Value: 100.0}Par_1: {Name: Second Parameter, Value: 200.0} Here is an example parameter string:\n{ 'Inputs': ['DT(us/m)', 'DTS(us/m)', 'RHOB(g/cc)'], 'Output': ['Near', 'Mid', 'Far'], 'ZSampMargin' : {'Value': [-10,10]}, 'StepOut' : {'Value': [1,1], 'Hidden': true}, 'Near Angle (deg)' : {'Type': 'Number', 'Value' : 6.0}, 'Mid Angle (deg)' : {'Type': 'Number', 'Value' : 20.0}, 'Far Angle (deg)' : {'Type': 'Number', 'Value' : 40.0}, 'Wavelet' : {'Type': 'File','Value': 'Seismics/*.wvlt'}, 'Method' : { 'Type': 'Select', 'Options': ['akirich', 'fatti'], 'Value': 'fatti'}, 'Help' : 'https://gist.github.com/waynegm/84f323ec4aab3961c23d' } Tips and Tricks Cross Platform Setup Prior to release 5.10 and 6.0.0pre7-1 any attribute set containing external attributes could not be shared between Windows and Linux because the Interpeter and External File input fields contain platform specific file paths.\nRelease 5.10 and 6.0.0pre7-1 introduced an optional mechanism to support cross platform attribute sets containing external attributes by using environment variables. Consider the following script for starting OpendTect on Linux:\n#!/bin/csh -f setenv DTECT_SETTINGS \"$HOME/.od6\" setenv OD_USER_PLUGIN_DIR \"$HOME/.od6\" setenv EX_PYTHON \"/opt/anaconda3/bin/python\" setenv OD_EX_DIR \"$HOME/Development/GIT_AREA/OpendTect-External-Attributes/\" /opt/seismic/OpendTect_6/6.0.0/start_dtect And an equivalent Windows command file:\n@set OD_USER_PLUGIN_DIR=%HOMEPATH%\\od6 @set EX_PYTHON=C:\\Miniconda3\\python.exe @set OD_EX_DIR=E:\\Development\\GIT_AREA\\OpendTect-External-Attributes\\ start \"\" \"C:\\Program Files\\OpendTect_6\\6.0.0\\bin\\win64\\Release\\od_start_dtect.exe\" The environment variable EX_PYTHON points to the python interpreter for each platform and entering %EX_PYTHON% into the Interpreter input field ensures the platform appropriate interpreter is used. Any name can be used for the environment variable.\nThe environment variable OD_EX_DIR points to a root folder below which the attribute script files can be found. The setting in the Linux startup script points to a Linux folder. The corresponding setting in the Windows command file points to the same location via a network share. This environment variable name is hard wired into the code so this variable name cannot be changed. Note that the script files cam be located in subfolders of the OD_EX_DIR folder.\nAttribute sets created by release 5.0.10 and 6.0.0pre7-1 and later that use these environment variables will not work in earlier versions of the External Attribute plugin.\nOpendTect (Linux) hangs after selecting a Python external attribute in the Attribute Description Editor This can happen if the Python file has Windows/DOS linebreaks. Use the dos2unix command on the Python file and all should be ok.\nSetting up a Python/Numpy/Scipy environment The Python environments provided by the OpendTect Installation Manager, specifically the MKL and Cuda environments, will work with this plugin although some scripts may require installation of additional packages. Any additional dependencies will usually be described in a README.md file next to the script, in the comments/description at the top of the script file or in the documentation.\nAlternatively you can install your own Python/Numpy/Scipy development stack for Python 3 from scratch. Continuum Analytics provide free Python installers for Linux and Windows in Anaconda. There is also a smaller DIY option called Miniconda which allows you to select just the packages you need.\nIn general you require Python 3 (\u003e=3.7) and compatible Numpy and Scipy. Numba is also highly recommended and required by some of the supplied scripts.\nHow it Works The following describes how the plugin works. Most of the details are handled by the extattrib.py module included in the plugin distribution. Python scripts need to import this module, provide a description of the User Interface in the xa.params Python dictionary and a doCompute function that implements the attribute calculation.\nWhen invoked with a commandline argument of -g the application should write out a JSON parameter string to stdout describing the attribute parameters and exit. When invoked with a commandline argument of -c json-parameter-string the application should read and parse the contents of json-parameter-string to get the attribute parameters read a 40 byte block of binary data from stdin called the SeismicInfo block (described below) start an endless loop that: reads a 16 byte block of binary data from stdin called the TraceInfo block (described below) reads a data block of 4 byte binary floats from stdin that contains the seismic trace data. The size of the data block depends on the content of the SeismicInfo (number of traces and number of inputs) and TraceInfo ( number of samples) blocks number_of_inputs * number_of_traces * number_of_samples * 4 bytes. calculates the attribute output writes a data block of 4 byte binary floats to stdout that contains the attribute output. The size of the output data block depends on the content of the TraceInfo ( number of samples) block and the number of output attributes number_of_samples * number_of_outputs * 4 bytes. SeismicInfo Block This block of binary data is written to the applications stdin immediately after it is started with the -c argument. It consists of 40 bytes as follows:\nSIZE FORMAT INDEX DESCRIPTION 4 bytes integer nrtraces number of traces for each input attribute 4 bytes integer nrinput number of input attributes 4 bytes integer nroutput number of output atrributes 4 bytes integer nrinl number of inline traces in the input data block 4 bytes integer nrcrl number of crossline traces in the input data block 4 bytes float zstep trace sampling interval (result of OpendTect API call SI().zstep()) 4 bytes float inldist distance between inlines (result of OpendTect API call SI().inlDistance()\u003c/em.) 4 bytes float crldist distance between crosslines (result of OpendTect API call SI().crlDistance()) 4 bytes float zFactor (result of OpendTect API call zFactor()) 4 bytes float dipFactor (result of OpendTect API call dipFactor()) TraceInfo Block This block of binary data is written to the application stdin immediately before each block of trace data. It consists of 16 bytes as follows:\nSIZE FORMAT INDEX DESCRIPTION 4 bytes integer nrsamp number of samples in each trace within the input data block (OpendTect nrsamples parameter) 4 bytes integer z0 position of first sample in data trace within entire seismic trace ( OpendTect z0 parameter) 4 bytes integer inl inline number of current calculation position 4 bytes integer crl crossline number of current calculation position ","categories":["docs"],"description":"seismic attributes in Python","excerpt":"seismic attributes in Python","ref":"/WMPlugin-Docs/docs/plugins/externalattrib/","tags":["ExternalAttrib","python"],"title":"External(Python) Attributes"},{"body":"Script: ex_lpa_smooth.py\nDescription This Python External Attribute script can be used to filter noise while preserving steep dips. A region of data around each sample location is approximated by a second order 3D polynomial using gaussian weighted least squares.\nThe approximation has the following form:\n$$ r_0+ r_1 * x + r_2 * y + r_3 * z + r_4 * x^2 + r_5 * y^2 + r_6 * z^2 +r_7 * x * y + r_8 * x * z + r_9 * y * z $$\nwhere x (inline), y (crossline) and z (time/depth) are relative to the analysis location, ie the analysis location has x=y=z=0.\nThis attribute calculates and outputs only the \\(r_0\\) term of the local polynomial approximation. This provides a smoother version of the input with relatively minor smearing of steep dips and fault cuts. Increasing either the Weight Factor or size of the analysis volume (StepOut or Z window) increases the amount of smoothing.\nExamples Input Parameters LPA Smoothing external attribute input parameters\nNAME DESCRIPTION Z window (+/-samples) Specifies the extent of the analysis cube in the Z direction. Number of Z samples in cube will be \\(2 * Zwindow + 1\\). Stepout Specifies the inline and crossline extent of the analysis cube. Number of samples in each direction will be \\(2 * Stepout + 1\\). Weight Factor Determines the extent of the gaussian weight function used in the weighted least squares approximation. The standard deviation of the gaussian weight function \\((\\sigma)\\) is related to this value by: \\(\\sigma = min(2*Stepout, 2*Zwindow)*WeightFactor\\). A value of 0.15 gives near zero weight for points at the smallest extent of the analysis cube. References Anisotropic Multidimensional Savitzky Golay kernels for Smoothing, Differentiation and Reconstruction\nPolynomial Expansion for Orientation and Motion Estimation\n","categories":["docs"],"description":"structure preserving spatial filter by local polynomial approximation","excerpt":"structure preserving spatial filter by local polynomial approximation","ref":"/WMPlugin-Docs/docs/externalattributes/lpasmooth/","tags":["ExternalAttrib","filter"],"title":"LPA Smoothing"},{"body":"This plugin displays lines and polylines from a Geopackage database file over an [opendtect] 3D horizon.\nDescription The plugin adds a “Geopackage Display” item to the “Add” context menu of 3D Horizons in the scene tree. Selecting the item opens a dialog box for selecting the Geopackage file, the layer to display and the line color and width. Multiple layers can be displayed.\nGeopackage Display menu item\nNotes The plugin requires the survey to have a projection based CRS defined. This plugin is actually part of the [geopackageexport] plugin - see notes there as well GIS data draped on an OpendTect 3D horizon\n","categories":["docs"],"description":"display lines and polylines in a Geopackage database on an OpendTect 3D horizon","excerpt":"display lines and polylines in a Geopackage database on an OpendTect …","ref":"/WMPlugin-Docs/docs/plugins/geopackagedisplay/","tags":["Geopackage"],"title":"GeopackageDisplay"},{"body":"The 6.6.8 release of the wmPlugins includes a number of new Python external attribute scripts that use the PyLops linear operator library for seismic modelling and inversion.\nAssuming wmPlugins is installed using the OpendTect Installation Manager, the scripts will be in the bin/python/wmpy/PyLops folder of the OpendTect software folder and include:\nPyLops/ex_poststack_inversion.py PyLops/ex_poststack_relative_inversion.py PyLops/ex_poststack_modelling.py PyLops/ex_prestack_modelling.py PyLops/ex_make_1d_seismic.py Using these scripts requires a Python environment with the PyLops module and it’s dependencies installed. The section Installing PyLops describes the PyLops installation process.\nPyLops/ex_poststack_inversion.py This Python External Attribute script uses the pylops.avo.poststack.PoststackInversion operator to do post-stack seismic inversion. The output is either the log Acoustic Impedance (AI) volume or the residual error.\nThe inputs required are volumes of the seismic to be inverted, a background log AI model and the seismic wavelet. Note that the polarity of the seismic wavelet must match the data.\nThe following figures show inversion input and output for a 1D model created by the PyLops/ex_make_1d_seismic.py script.\nImpedance Model (red) and Background Model (blue)\nImpedance Model (red) and Seismic Model (blue)\nImpedance Model (red) and Inverted Impedance (blue)\nThere is also a PyLops/ex_poststack_relative_inversion.py script that runs the inversion without a background model:\nImpedance Model (red) and Inverted Relative Impedance (blue)\nInverted Relative Impedance Example\nInput Parameters ex_poststack_inversion.py input parameters\nNAME DESCRIPTION Output What to calculate - choice of Acoustic Impedance or the Residual Error. Regularization (%) A small amount of noise to add to stabilize the inversion. Wavelet An OpendTect wavelet file. Note the wavelet polarity must be consistent with the polarity of the data being inverted.\nPyLops/ex_prestack_modelling.py The PyLops/ex_prestack_modelling.py script uses the pylops.avo.avo.AVOLinearModelling operator to create a pre-stack angle volume from well data. The output is either an Aki-Richards or Fatti approximate reflectivity model filtered by a user specified wavelet.\nThe inputs required are 3 log data cubes with compressional sonic (DT in us/m), shear sonic (DTS in us/m) and density (RHOB in g/cc). These can be created from well log data using the “Create Log Cube” right mouse button context menu in the scene well tree or the “Processing|Create Seismic Output|From Well Logs” main menu. Also needed is a wavelet with the appropriate polarity for the data being modelled.\nThe generated synthetics can be displayed in the 3D window and compared with real angle stack data through the well location.\nPrestack modelling example\nInput Parameters ex_prestack_modelling.py input parameters\nNAME DESCRIPTION Angle (deg) The desired output angle volume. Method What approximation to use, Aki RIchards or Fatti. Wavelet An OpendTect wavelet file. Installing PyLops The PyLops Python package and it’s dependencies can be installed in an active conda environment using:\nconda install -c conda-forge pylops or\npip install pylops See the PyLops documentation for more information.\n","categories":["docs"],"description":"various external attributes that use the PyLops linear operator library","excerpt":"various external attributes that use the PyLops linear operator …","ref":"/WMPlugin-Docs/docs/externalattributes/pylops/","tags":["ExternalAttrib","pylops","inversion","modelling"],"title":"PyLops Attributes"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/docs/faq/","tags":"","title":"FAQ"},{"body":"This plugin, for the open source seismic interpretation platform [opendtect] Version 6.4.0 or later, exports [opendtect] data to a GeoPackage database. GeoPackage is an open, non-proprietary, platform-independent, self describing standards-based data format for geospatial data.\nDescription The plugin adds a “Geopackage Export” item to the Survey-Export main menu. Selecting the item opens a tabbed dialog box for selecting the various elements to export and the destination file name. The following table shows the [opendtect] data elements supported and the corresponding tables created in the exported GeoPackage database.\nOpendTect Item GeoPackage Table Name Geometry Notes Survey Box Survey Polygon Single attribute, the survey name 2D Line Geometry 2DLines LineString Single attribute, the line name 2D Line Stations 2DStations Points Two attributes, the line name and station number Random Lines RandomLines LineString Single attribute, the random line name Wells Wells Points Well surface location and 3 attributes, the well name, UWID and status (not currently set by OpendTect) Well Tracks WellTracks LineString Single attribute, the well name Well Markers’, ‘WellMarkers’, ‘Points Four attributes, well name, marker name, MD and TVDSS Open Polygons PolyLines LineString Single attribute, the polyline name. The z values of the polyline are not exported Closed Polygons Polygons Polygon Single attribute, the polygon name. The z values of the polygon are not exported 2D and 3D Horizons Set by the user Point Single attribute, the horizon z value in millisecs or metres depending on the Z domain of the survey Notes It is possible to append to an existing database. This is primarily used for exporting multiple horizons/attributes to the same GeoPackage. Appending does not overwrite items already in the GeoPackage it will just add another copy to the respective table. The plugin requires the survey to have a projection based CRS defined. On Windows the folder containing the plugin DLL’s must be added to the PATH environment variable either by editing the corresponding system variable (Control Panel\u003eSystem and Security\u003eSystem - Advanced system settings - Environment Variables) or adding a line like “@set PATH=%HOMEPATH%.od\\bin\\win64\\Release;%PATH%” (adjust “%HOMEPATH%.od\\bin\\win64\\Release” to reflect your installation) to the bat script used to start OpendTect. The GeoPackage format is supported by major GIS software packages. The following figure shows display of data exported from OpendTect using the plugin in the open source GIS package, QGIS. OpendTect data displayed in QGIS\nInput Parameters The dialog box associated with the plugin has a file entry control to select the output file, a check box to allow appending to the output file instead of overwriting and 8 tabs to select the content to export:\nSurvey tab 2D Lines tab Random Lines tab Wells tab Polylines tab Horizon tab ","categories":["docs"],"description":"export OpendTect data to a GeoPackage database","excerpt":"export OpendTect data to a GeoPackage database","ref":"/WMPlugin-Docs/docs/plugins/geopackageexport/","tags":["Geopackage","qgis"],"title":"GeopackageExport"},{"body":"Script: Filtering/ex_spatial_filter_circular.py\nDescription This Python External Attribute script applies lowpass, highpass, bandpass or band reject circularly symmetric spatial filters. The filters are applied by direct spatial convolution using the MAXFLAT operators described by Khan and Ohba (2001).\nFilter responses\nNote that the filter cutoff or band pass/reject frequency is specified in Normalised Spatial Frequency. Normalised Frequency ranges from 0 to 1 at the spatial nyquist. Converting a spatial frequency in cycles/metre to the equivalent Normalised Spatial Frequency is as simple as dividing it by the spatial nyquist frequency.\nAs written the filter does not take into account differences in spatial sampling in the inline and crossline directions.\nExamples This example uses a Low Pass filter to remove acquisition footprint from the F3 Demo dataset.\nFK spectrum of inline 425\nNote the noise at 0.0133 cycles/metre or 0.67 normalised frequency (i.e. 0.0133/0.02).\nA spatial low pass filter with a normalised frequency cutoff of 0.5 is very effective at attenuating this acquisition footprint.\nFK spectrum of inline 425 after filter\nInput Parameters ex_spatial_filter.py input parameters\nNAME DESCRIPTION Stepout Determines the size of the convolution operator. Minimum of 9 (filter kernel size of 19) recommended. Type Filter type - Low Pass, High Pass, Band Pass or Band Reject. Normalised Spatial Frequency For Low Pass and High Pass filters this specifies the filter cutoff. For Band Pass/Reject filters this is the centre of the pass/reject band. The width of the band is hardwired to +/- 0.1 in the script. The filter cutoff corresponds to the half amplitude point. ","categories":["docs"],"description":"lowpass, highpass, bandpass or band reject circularly symmetric spatial filters","excerpt":"lowpass, highpass, bandpass or band reject circularly symmetric …","ref":"/WMPlugin-Docs/docs/externalattributes/spatialfiltercircular/","tags":["ExternalAttrib","filter"],"title":"Spatial Filter - Circular"},{"body":"This plugin, for the open source seismic interpretation platform [opendtect] Version 6.4.0 or later, exports [opendtect] 3D horizon and attribute data to a GeoTIFF image. GeoTIFF is a public domain metadata standard which allows georeferencing information to be embedded within a TIFF image file. GeoTIFF image files are widely supported by GIS software.\nDescription The plugin adds a “Geotiff Export” item to the Survey-Export main menu. Selecting the item opens a dialog box for selecting the 3D horizon and attributes to export and the destination file name. The horizon Z values and attributes values are exported to Float32 bands in the output image. The plugin supports exporting multiple attributes to a single GeoTIFF file, however some GIS packages may not have the flexibility to display individual bands from a Float32 multiband image (eg QGIS v3.6). If multiband images prove to be a problem the option exists to run the plugin multiple times and save each attribute to a separate GeoTIFF file.\nNotes The plugin requires the survey to have a projection based CRS defined. This plugin is actually part of the [geopackageexport] plugin - see notes there as well OpendTect 3D horizon data displayed in a QGIS print layout\nInput Parameters ","categories":["docs"],"description":"export OpendTect 3D horizon and attribute data to a GeoTIFF image","excerpt":"export OpendTect 3D horizon and attribute data to a GeoTIFF image","ref":"/WMPlugin-Docs/docs/plugins/geotiffexport/","tags":["Geopackage","qgis","geotif"],"title":"GeotiffExport"},{"body":"Script: Filtering/ex_spatial_filter_rectangular.py\nDescription This Python External Attribute script applies lowpass, highpass, bandpass or band reject spatial filters with rectangular symmetry. The filters are applied by direct spatial convolution of a kernel formed by cascading two 1D Hamming Window FIR filters computed using scipy.signal.firwin (https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.firwin.html).\nBy setting the stepout in one direction to 0 the filter will be applied as a 1D spatial filter in the other direction.\nNote that the filter cutoff or band pass/reject frequency is specified in Normalised Spatial Frequency. Normalised Frequency ranges from 0 to 1 at the spatial nyquist. Converting a spatial frequency in cycles/metre to the equivalent Normalised Spatial Frequency is as simple as dividing it by the spatial nyquist frequency.\nExamples This example shows inline and crossline FK spectra after applying a 2D lowpass rectangular filter with inline and crossline normalised frequency cutoffs of 0.6 and 0.3 respectively.\nThis example shows a timeslice at 300ms TWT from the F3 Demo dataset after applying a 1D spatial filter along the inlines (stepout of 0,9) with a crossline normalised frequency cutoff of 0.5.\nThis example shows the timeslice at 300ms TWT from the F3 Demo dataset with the crossline lowpass filter above followed by a 1D bandreject spatial filter along the crosslines (stepout 9,0) with an inline normalised rejection frequency of 0.17.\nInput Parameters ex_spatial_filte_rectangular.py input parameters\nNAME DESCRIPTION Stepout Determines the size of the convolution operator. Minimum of 9 (filter kernel size of 19) recommended. Setting the stepout to zero will apply a 1D filter, e.g. a stepout of 0,9 will apply a 1D crossline frequency filter. Type Filter type - Low Pass, High Pass, Band Pass or Band Reject. Normalised Inline Spatial Frequency For Low Pass and High Pass filters this specifies the inline filter cutoff. For Band Pass/Reject filters this is the centre of the pass/reject band. The width of the band is hardwired to +/- 0.1 in the script. The filter cutoff corresponds to the half amplitude point. Normalised Xline Spatial Frequency For Low Pass and High Pass filters this specifies the crosslineline filter cutoff. For Band Pass/Reject filters this is the centre of the pass/reject band. The width of the band is hardwired to +/- 0.1 in the script. The filter cutoff corresponds to the half amplitude point. ","categories":["docs"],"description":"lowpass, highpass, bandpass or band reject rectangular spatial filters","excerpt":"lowpass, highpass, bandpass or band reject rectangular spatial filters","ref":"/WMPlugin-Docs/docs/externalattributes/spatialfilterrectangular/","tags":["ExternalAttrib","filter"],"title":"Spatial Filter - Rectangular"},{"body":"This attibute plugin for the open source seismic interpretation platform [opendtect] Version 6.0.0 or later calculates inline, crossline and time/depth gradients.\nDescription This plugin calculates the inline, crossline or time/depth gradient using operators optimised for rotation invariance, ie equal response in all directions, proposed by Kroon (2009) and Farid and Simoncelli(2004). These provide more accurate alternatives to the Prewitt filter option of the OpendTect Convolve attribute for computing gradients.\nThe attribute offers a choice of Kroon’s 3x3x3, Farid and Simoncelli’s 5x5x5 or Farid and Simoncelli’s 7x7x7 operator. The following figures demonstrate the relative accuracy of these operators and the OpendTect Prewitt filter on a simple periodic signal (top left) with event dip angle shown top right. Gradients calculated using each operator are used to compute the event dip angle and the absolute dip angle error. The superior accuracy of the operators provided by this attribute is clear.\nAccuracy of the gradient attribute operators\nInput Parameters This attribute has 3 parameters:\nNAME DESCRIPTION Input Volume The input attribute volume. Output Gradient What to calculate - choice of Inline, Crossline or Z gradient. Operator What operator to use - choice of Kroon's 3x3x3, Farid's 5x5x5 or Farid's 7x7x7. Gradient attribute input parameter dialog\n","categories":["docs"],"description":"calculate inline, crossline and time/depth gradients","excerpt":"calculate inline, crossline and time/depth gradients","ref":"/WMPlugin-Docs/docs/plugins/gradientattrib/","tags":["GradientAttrib","filter"],"title":"GradientAttrib"},{"body":"This plugin for the open source seismic interpretation platform [opendtect] Version 6.0 or later is a collection of “Local Attributes” based on the work of [Fomel (2007) and others](https://library.seg.org/doi/abs/10.1190/1.2437573\"Local seismic attributes. Sergey Fomel. GEOPHYSICS 2007 72:3\") for the Madagascar package. At this point only spectral (time-frequency) decomposition using local attributes (previously called the LTFAttrib) is implemented. The predecessor to this plugin (LTFAttrib, now obsolete) had a hard dependency on the Madagascar libraries and was consequently only available for Linux. This dependency has been removed and the plugin is now available for both Linux and Windows.\nSpectral (Time-frequency) Decomposition by Local Attributes Formerly called the LTFAttrib, this is an implementation of the method of local time-frequency analysis as described by Liu, G etal (2011). This decomposition uses least-squares inversion with shaping regularization to estimate the time-varying Fourier coefficients. Its principal advantages over the STFT (short time fourier transform) is it much quicker if only a few frequencies are required and it is generally more accurate for short window lengths.\nExamples The output of the LTF attribute (ltf30) is visually identical and also highly correlated to the OpendTect FFT spectral decomposition (sdfreq30) as shown in the following crossplot of the two attributes.\nCrossplot of LTFAttrib vs FFT Spectral Decomposition\nInput Parameters This attribute has 4 parameters:\nNAME DESCRIPTION Input Volume The attribute volume to be analysed. Time Gate Where the attribute is calulated. The time gate does not have to be symmetric. The time gate length is used as the smoothing radius for shaping regularization.The value from the gate centre is output - useful for analysing a zone offset from an horizon. Recommend setting the gate length equal to or less than the FFT window length you would used for the standard OpendTect FFT spectral decomposition. Frequency The frequency component(s) to estimate. Iterations The number of inversion iterations. LTF Attributes input parameters\n","categories":["docs"],"description":"various attributes based on Fomel's local attribute algorithm from the Madagascar package","excerpt":"various attributes based on Fomel's local attribute algorithm from the …","ref":"/WMPlugin-Docs/docs/plugins/localattrib/","tags":["time-frequency","spectral-decomposition"],"title":"LocalAttrib"},{"body":"Script: ex_vector_filter_dip.py\nDescription This External Attribute script can be used to apply a vector filter to orientation ( inline and crossline dip) data. The script offers a choice of mean vector, L1 vector median and L2 vector median filters.\nInitially the inline and crossline dip data are converted to a normal vector to the local orientation: \\([x_i, y_i, z_i]\\).\nThe Mean Vector Filter averages each of the vector components of the orientation normal vectors in the analysis cube: $$ \\Big[x_f, y_f, z_f\\Big] = \\frac{1}{N} \\Big[\\sum\\limits_{i}^N x_i, \\sum\\limits_{i}^N y_i, \\sum\\limits_{i}^N z_i\\Big] $$\nThe L1 vector filter finds the normal vector in the analysis cube whose sum of absolute distance from all the others is a minimum: $$ \\Big[x_f, y_f, z_f\\Big] = argmin \\sum\\limits_{i}^N \\Big[|x_f-x_i| + |y_f-y_i| + |z_f-z_i|\\Big] $$\nThe L2 vector filter finds the normal vector in the analysis cube whose sum of squared distance from all the others is a minimum: $$ \\Big[x_f, y_f, z_f\\Big] = argmin \\sum\\limits_{i}^N \\Big[(x_f-x_i)^2 + (y_f-y_i)^2 + (z_f-z_i)^2\\Big] $$\nThe filtered orientation can be output as any of the following:\nOUTPUT DESCRIPTION Inline Dip Event dip observed on a crossline in microseconds per metre for time surveys and millimetres per metre for depth surveys. Output can be positive or negative with the convention that events dipping towards larger inline numbers producing positive dips. Crossline Dip Event dip observed on an inline in microseconds per metre for time surveys and millimetres per metre for depth surveys. Output can be positive or negative with the convention that events dipping towards larger crossline numbers producing positive dips. True Dip Event dip in microseconds per metre for time surveys and millimetres per metre for depth surveys. Output is always positive. Dip Azimuth Azimuth of the True Dip direction relative to the survey orientation. Output ranges from -180 to 180 degrees. Positive azimuth is defined from the inline in the direction of increasing crossline numbers. Azimuth = 0 indicates that the dip is dipping in the direction of increasing crossline numbers. Azimuth = 90 indicates that the dip is dipping in the direction of increasing inline numbers. The script requires the Numba Python package. Examples Unfiltered phase dip - crossline dip on an inline\nMean vector filtered phase dip - 3x3x3 (Stepout and ZWindow of 1)\nL1 vector median filtered phase dip - 3x3x3 (Stepout and ZWindow of 1)\nL2 vector median filtered phase dip - 3x3x3 (Stepout and ZWindow of 1)\nInput Parameters ex_vector_filter_dip.py input parameters\nNAME DESCRIPTION Output What to calculate - choice of inline dip, crossline dip, true dip or dip azimuth. Z window (+/-samples) Specifies the extent of the analysis cube in the Z direction. Number of Z samples in cube will be \\((2*Zwindow+1)\\). Stepout Specifies the inline and crossline extent of the analysis cube. Number of samples in each direction will be \\((2*Stepout+1)\\). Filter Choice of Mean Dip, L1 Vector Median or L2 Vector Median. ","categories":["docs"],"description":"vector filter orientation (inline and crossline dip) data","excerpt":"vector filter orientation (inline and crossline dip) data","ref":"/WMPlugin-Docs/docs/externalattributes/vectorfilter/","tags":["ExternalAttrib","filter"],"title":"Vector Filter"},{"body":"This plugin, for the open source seismic interpretation platform [opendtect] Version 6.4.0 or later, creates a 3D horizon/grid from the 2D and 3D horizon interpretation in an [opendtect] survey/project.\nDescription The plugin adds a “Grid 2D-3D Horizon…” item to the “Processing|Create Horizon Output” menu which opens a dialog box for:\nSelecting 2D and 3D seismic interpretation and contour poly-lines to grid Specifying the output horizon name, extent and inline /crossline interval Specifying a cropping polygon Specifying a set of fault polygon constraints Selecting the interpolation algorithm to use In the current release the plugin includes 4 gridding/interolation algorithms:\nLocal Thin Plate Splines (LTPS) Multilevel B-Splines (MBA) Inverse distance squared weighted averages (IDW) Nearest neighbour (NRN) The LTPS and IDW methods can apply fault constraints (heave polygons) to ensure the input data used to interpolate a grid node is from the same side of any faults.\nInput Parameters Grid 2D-3D horizon plugin input data selection dialog\nGrid 2D-3D horizon plugin input dialog, showing Bounding Box grid extent and continuous curvature interpolation settings\nGrid Extent: Four options available: Range - explicitly set the Inl/Crl range and steps for the output surface Bounding Box - set the output horizon extent from the extent of the input data, adjusted to snap to the specified Inl/Crl steps Convex Hull - sets the gridding scope to the convex hull of the input data, adjusted to snap to the specified Inl/Crl steps Horizon - set the output horizon extent to match an existing horizon in the survey/project Cropping Polygon - select a polygon to crop the grid output Fault Polygons - select polygons to use as fault constraints during gridding. Suggest fault naming scheme that prefixes the polygons with “f_horizon name” to make them easy to select Algorithms Local Thin Plate Spline This is a multi-stage algorithm with the following steps:\nInterpolate the data to the corners of the surrounding grid nodes (ie nearest neighbour) Fit a Multilevel B-Spline (MBA) surface (no fault constraints applied) and compute residuals from the input Grid the MBA residuals using a radial basis function algorithm using a thin plate spline basis. The points used in the basis fit are determined using distance and azimuth from the grid point. The data around each grid point is sorted based on distance and binned into one of eight azimuth sectors. Only the nearest points in each sector are used in the interpolation. No interpolation is done if there are less than 5 sectors with data. Fault constraints are used in this stage. The residuals are added to the MBA stage output to produce the final surface. Controls are the maximum search radius and the maximum points used from each sector. This algorithm can use fault constraints.\nExample of Local Thin Plate Spline gridding\nMultilevel B-Spline This is the adaptive Multi-level B-Spline algorithm published by Lee, Wolberg and Shin (1997) as implemented by Denis Demidov.\nThis method does not use fault contraints.\nExample of Multilevel B-Spline gridding\nInverse Distance Weighted This is basic Inverse Distance Squared Weighted gridding. By tweaking the parameters it is possible to control the input data used for each grid point. Options are:\nNo distance or point count limit (global) Use all points within a specified distance Use nearest number of points Limit by both the distance and the nearest number of points The method can use fault constraints but will be very slow unless appropriate distance and point count limits are set.\nExample of IDW gridding\nExample of IDW gridding using only a distance limit\nExample of IDW gridding using only a point count limit\nExample of IDW gridding using both distance and point count limits\nNearest Neighbour This uses a simple weighted average method to assign grid values to the 4 nodes around each input data point.\nExample of Nearest Neighbour gridding\n","categories":["docs"],"description":"creates a 3D horizon/grid from 2D and 3D horizon interpretation","excerpt":"creates a 3D horizon/grid from 2D and 3D horizon interpretation","ref":"/WMPlugin-Docs/docs/plugins/grid2d-3d/","tags":["gridding"],"title":"Grid 2D-3D Horizon"},{"body":" This plugin, for the open source seismic interpretation platform [opendtect] Version 6.4.0 or later, allows creation and editing of a file with Z shift, phase rotations and amplitude scaling corrections for 2D and 3D seismic in an [opendtect] survey/project. The plugin also includes an attribute (Mistie Application) that will apply the corrections.\nIn an ideal world we would be given 2D seismic data that has consistent Z, phase and amplitude scales. In the real world this doesn’t always happen and 2D seismic interpretation projects accumulate inconsistencies as more data is added. The concept implemented by this plugin is the interpreter builds/maintains the correction table as they work through the data. The virtual corrected seismic from the Mistie Application attribute can be interpreted on the fly or the interpreter can generate a new adjusted dataset and interpret that.\nDescription The plugin provides components to:\nEstimate misties from seismic data or horizon interpretation Analyse the misties and estimate corrections to minimize the misties in a least squares sense Edit/Maintain a set of mistie corrections Attribute to apply the corrections to seismic data and tools to apply Z corrections to horizon interpretation. There are two alternative workflows to build a mistie correction file:\nEstimate the misties from the data and compute a mistie correction file in the Mistie Analysis dialog Manually build the mistie correction file in the Mistie Correction Editor Mistie Analysis The plugin adds a “Mistie Analysis” item to the Analysis main menu. Selecting the item opens the Mistie Analysis dialog:\nMistie Analysis\nActions associated with the toolbar buttons are:\nICON DESCRIPTION Open the Mistie Estimation dialog to estimate misties from the seismic data. Existing contents of the mistie table are erased. Load mistie estimates from a previously saved file. Existing contents of the mistie table are erased. Save the current mistie estimates. If the current misties were loaded from a file then that will be overwritten, otherwise user will be asked to provide a new file name. Prompts for a file and saves the current mistie estimates. Prompts for another mistie file and merges the results with the mistie set currently active in the tool optionally keeping or replacing common items. Opens a dialog to filter the misties listed in the table, used for correction calculation and displayed in the mistie report. Open the Correction Calculation dialog. Opens a web browser and displays an interactive dashboard report for the current misties and correction set active in the tool. Mistie Estimation The toolbar item in the Mistie Analysis dialog opens a dialog box where the user can specify whether misties should be estimated from the seismic data or from horizon interpretation.\nIn the case of estimation from the seismic data the dialog looks like:\nMistie Estimation from Seismic\nThe user can:\nSelect the data type (attribute) and which lines to include in the analysis Include and select a 3D seismic volume to include in the analysis Specify the trace interval along 2D lines to estimate the 2D to 3D misties, the average mistie is assigned to the 2D to 3D tie Limit the maximum time-shift or mistie to consider Specify a Z window for the cross correlation of traces at line interections Indicate if only time corrections should be estimated (default is to simultaneously estimate Z, phase rotation and amplitude corrections) The default estimation of Z phase rotation and amplitude misties is based on the method described by Bishop and Nunns (1994) using cross correlation of the amplitude envelope of the traces. This is the preferred method for seismic data with a reasonable bandwidth. For narrow bandwidth data (eg site survey data) the time corrections only estimation method which uses cross correlation of the raw seismic traces may generate better results.\nIn the case of estimation from horizon interpretation the dialog looks like:\nMistie Estimation from Horizon Interpretation\nThe user can:\nSelect a 2D horizon and set of lines to use Select a 3D horizon to use Specify the trace interval along 2D lines to estimate the 2D to 3D misties, the average mistie is assigned to the 2D to 3D tie The Apply button will initiate the estimation of the misties, when complete results are displayed in the Mistie Analysis dialog. Clicking the icon will generate a Mistie Report with histograms and scatterplot of the mistie estimates for review.\nMistie Filter The toolbar item in the Mistie Analysis dialog opens the following dialog that allows filtering of the misties displayed in the list, used for correction calculation and included in the mistie report:\nMistie filtering\nActivate the checkbox next to the filter to activate it. At this moment only filtering by the tie quality (correlation coefficient) is supported. The tie quality can vary from 0.0 (no tie) to 1.0 (perfect tie). A cut-off of 0.5-0.6 should filter unreliable mistie estimates.\nMistie Correction Calculation The toolbar item in the Mistie Analysis dialog opens the following dialog to compute mistie corrections that minimize the root mean square (RMS) mistie after correction:\nMistie correction calculation\nThe user can:\nOptionally select one or more line(s) to use as a reference. Reference lines will have Z, phase and amplitude corrections constrained to be 0, 0, 1 respectively and corrections will be computed only for the non-reference lines. Selecting no lines will distribute corrections across all lines. Control the maximum number of iterations used to calculate the corrections - the default value should be adequate in most circumstances. Control the convergence criteria that stops the iteration used to calulate the corrections - iterations stop if the change in the RMS mistie (after applying corrections) between successive iterations is less than the threshhold. Thresholds exist for the Z, phase and amplitude corrections. The default values should be adquate in most circumstances. The calculated corrections will be displayed in a new table dialog:\n{{ figure(‘mistie_correction_viewer.jpg’, ‘Mistie correction viewer’) }}\nThe toolbar buttons can be used to save the corrrections to a disk file.\nMistie Report The toolbar item in the Mistie Analysis dialog generates and displays in the system web browser a graphical, html format dashboard of the mistie anaysis and correction calculation results. This includes tabulated data, histograms and a 3D scatterplot. Here is an example of a mistie report. The html file displayed in the browser can be saved for later display suing the browser’s save page functionality.\nMistie Correction Editor The plugin adds a “Mistie Corrections” item to the Survey-Manage main menu. Selecting the item opens the Mistie Correction Editor. This tool can be used to manually create mistie correction files or modify files generated by the Mistie Correction Calculation.\nMistie correction editor\nThe editor has toolbar buttons to:\nICON DESCRIPTION Create a mistie correction set with all the 2D lines in the project (with default corrections that make no change to the data). Existing contents of the mistie table are erased. Load mistie corrections from a previously saved file. Existing contents of the mistie table are erased. Save the current mistie corrections. If the current corrections were loaded from a file then that will be overwritten, otherwise the user will be asked to provide a new file name. Prompts for a file and saves the current mistie corrections. Merge another mistie correction set file with the current set, optionally keeping or replacing the existing corrections where there is duplicate line/dataset names. Within the editor it is possible to:\nAdd new lines/datasets and associated corrections Edit existing corrections RightMouseButton click on a row brings up a menu to insert or delete selected row(s) Limited clipboard copy and paste is available You can include corrections for a particular 3D seismic volume by using a line/dataset name of the form “3D_XXXX” where the “XXXX” is the volume name, eg note the “3D_pstm” in the above figure. Multiple unique “3D_XXXX” entries are allowed to specify corrections for different 3D volumes in the project.\nIf a line doesn’t need a correction it can be omitted from the set and default (no change) corrections will be assumed when the Mistie Application attribute is applied. A message (which can be safely ignored) is added to the log file when this occurs.\nThe name in the line/dataset column must exactly match the project line name.\nMistie Application Attribute The plugin adds a “Mistie Application” attribute to the list of [opendtect] attributes.\nMistie Application attribute input parameters\nThe attribute parameters include:\nThe input seismic volume to be corrected A file with the mistie correction set to apply Which corrections to apply When the attribute is displayed the shift, phase rotation and amplitude scalar for the line is read from the correction file and applied.\nMistie Correction of Horizon Interpretation The plugin adds an “Apply Mistie Corrections” item to the “Processing | Create Horizon Output” main menu that opens a dialog box to apply mistie corrections to 2D and 3D horizon interpretation.\nMistie Application to 2D Horizon\nMistie Application to 3D Horizon\nGeneral Notes The default file extension for saved mistie estimates is “mistie” The default file extension for saved mistie corrections is “miscor” The default file location for all files is the Misc folder in the OpendTect survey/project folder All files are in a simple text format which can also be modified using a text editor ","categories":["docs"],"description":"edit/apply Z shifts, phase rotations and amplitude corrections to seismic data","excerpt":"edit/apply Z shifts, phase rotations and amplitude corrections to …","ref":"/WMPlugin-Docs/docs/plugins/mistie/","tags":["Mistie"],"title":"Mistie"},{"body":"Script: Miscellaneous/ex_correlation.py\nDescription This Python External Attribute script provides an alternative to the builtin OpendTect Match Delta attribute to measure time shifts between similar events in different seismic volumes. This script uses local normalised cross correlation to determine the relative Z shift between 2 data volumes. Aside from the estimate of relative Z shift the attribute can also output the correlation value. The correlation value which ranges from 0 (low correlation) to 1 (high correlation) provides a quantitative assessment of the reliability of the Z shift estimate.\nThe script requires the Numba Python package.\nExamples This example provides a comparison of this external attribute script (left) with the Match Delta attribute (right) for a depth section and itself shifted up by 13 metres. The Match Delta attribute output is much noisier albiet it can be calculated much quicker.\nThis example shows the Correlation Quality output for the same data as above.\nCorrelation quality**\nInput Parameters ex_correlation.py input parameters**\nNAME DESCRIPTION Output What to calculate - choice of the Z shift in millisecs or metres or the corresponding correlation coefficient. Z window (+/-samples) This in conjunction with the Max Lag parameter determines the length of the segments cross correlated. \\(SegmentLength = 2*(Zwindow - MaxLag)+1\\). Max Lag (samples) Specifies maximum number of samples to search for the maximum correlation. Note the user has to ensure that Z window is greater than Max Lag otherwise the script will exit with errors.\n","categories":["docs"],"description":"measure time shifts between volumes using correlation","excerpt":"measure time shifts between volumes using correlation","ref":"/WMPlugin-Docs/docs/externalattributes/zdelayest/","tags":["ExternalAttrib","attribute"],"title":"Time Delay Estimation"},{"body":"Script: Miscellaneous/ex_zc_block.py\nDescription This Python External Attribute script blocks a seismic trace between zero crossings. The block amplitude is determined by the min/max of the interval blocked.\nThe script requires the Numba Python package.\nExamples This example shows the attribute output (black wiggle) over the input (variable density). To get a blocky wiggle display interpolation has to be turned off in the 2D viewer properties. Zero crossing block\nInput Parameters ex_zc_block.py input parameters\nThere are no input parameters other than selection of the input volume.\n","categories":["docs"],"description":"blocks a seismic trace between zero crossings","excerpt":"blocks a seismic trace between zero crossings","ref":"/WMPlugin-Docs/docs/externalattributes/zcblock/","tags":["ExternalAttrib","filter"],"title":"ZC Block"},{"body":"This attribute plugin for the open source seismic interpretation platform [opendtect] applies a structure preserving Mean of Least Variance filter.\nDescription This attribute is an implementation of a mean of least variance filter Schulze \u0026 Pearce (1993) where the analysis elements are all the possible planes through the sample points in the analysis block. This algorithm may be similar to that proposed by Al-Dossary \u0026 Wang (2011).\nThe sample variance for all samples on each analysis element is calculated and the output statistic (average, mean or element index) is output for the element with the least variance.\nThe following figure shows the relationship between the geometry of the analysis elements and the element index.\nMLV Filter analysis elements\nExamples MLV 3 elements\nInput Parameters This attribute has 2 parameters: NAME DESCRIPTION Filter size Specifies a cube of samples centred at the analysis location. Increasing the size will increase the degree of smoothing at the risk of smearing structural features. As the examples show it may be better to apply multiple passes of a small size filter than a single pass of a larger filter to reduce the risk of artifacts in the output. OpendTect makes it really easy to cascade multiple filter passes. Output statistic What the filter will output. The options are average, median or the element index. The element index is included for curiosity and quality control. Generally the default Average provides the most pleasing output. MLF Filter input parameters\n","categories":["docs"],"description":"structure preserving spatial filter by mean of least variance filter","excerpt":"structure preserving spatial filter by mean of least variance filter","ref":"/WMPlugin-Docs/docs/plugins/mlvfilter/","tags":["MLVFilterAttrib","filter"],"title":"MLVFilterAttrib"},{"body":"This attribute plugin for the open source seismic interpretation platform [opendtect] Version 6.0.0 or later performs spectral (time-frequency) decomposition using a recursive filter.\nDescription This plugin can be used as an alternative to the OpendTect FFT spectral decomposition attribute.\nIt does spectral decomposition using Nilsen’s (2007) time-frequency analysis algorithm which is a recursive filter approximation to a special case of the short time fourier transform (STFT).\nThe primary advantage of this plugin over the standard OpendTect FFT spectral decomposition is that it can be evaluated significantly faster. As an example, under Linux on an Intel Core i5 for a 2000 sample per trace dataset, this attribute can generate a single frequency cube at 4000 traces per second. This is considerably faster than the 140-150 traces per second achieved when applying the OpendTect FFT spectral decomposition attribute. This processing speed advantage is reduced as the number of output frequencies increases but in this test case it still remains substantially faster even for output of up to 30 frequencies. This algorithm is also more accurate for shorter windows.\nAs of release 6.4.8 the plugin has been extended to optionally include time-frequency reassignment. In normal spectral decomposition a window/gate is slid along the trace and the amplitude of each frequency component is assigned to the middle of the gate. This blurs/smooths the decomposition along both the time and frequency axis. While shortening the gate can improve the apparent time resolution, it has the side effect of increased blurring/smoothing,i.e. lower resolution, in frequency. Reassignment uses information in the phase spectrum to relocate spectral energy to its origin in the time gate and on the frequency axis. This produces significantly sharper time-frequency spectra. However reassignment becomes ambiguous when more than one signal component is present and they are close or overlap in time or frequency. To perform the reassignment the decomposition must be done for a range of frequencies so execution time will be slower than without reassignment.\nExamples The output of the RSpec attribute (rfreq30) is visually identical and also highly correlated to the OpendTect FFT spectral decomposition (sdfreq30) as shown in the following crossplot of the two attributes. Crossplot of RSpecAttrib vs FFT Spectral Decomposition\nHere is an inline display from the F3 demo dataset comparing the 24Hz component for a +/-16ms gate for all 4 methods. The enhanced time resolution of the reassigned decomposition is obvious.\nComparison of various decompositions on an inline\nHere is the 24Hz component on the MSF4 horizon for all 4 methods. Channels are better defined on the Recursive and Local Attribute decompositions relative to the OpendTect spectral decomposition attribute. The reassigned decomposition has a radically different appearance due to its sharpness in time and frequency but the significance of the variations is unknown.\nComparison of various decompositions on an horizon slice\nInput Parameters This attribute has 4 parameters: NAME DESCRIPTION Input Volume The attribute volume to be analysed. Time/Depth Gate This determines the position and time resolution of the analysis. The value from the gate centre is output - useful for analysing a zone offset from an horizon. Recommend setting the gate length equal to or less than the FFT window length you would used for the standard OpendTect FFT spectral decomposition. Output frequency When displaying the attribute in the tree this is the frequency slice that will be generated. Step This determines the set of frequencies that can be chosen when generating a frequency volume. With Reassignment Toggle on to compute the reassigned decomposition RSpecAttrib input parameter dialog\n","categories":["docs"],"description":"spectral (time-frequency) decomposition using recursive filters","excerpt":"spectral (time-frequency) decomposition using recursive filters","ref":"/WMPlugin-Docs/docs/plugins/rspecattrib/","tags":["time-frequency\"","spectral-decomposition"],"title":"RSpecAttrib"},{"body":"This plugin provides various tools to enhance the open source seismic interpretation platform [opendtect] Version 6.4.0 or later.\nConvex Hull Polygon Description Generate a convex hull polygon covering either the extent of 2D/3D seismic coverage or 2D/3D horizon interpretation. The user has full control over the 2D and 3D data included. The polygon can be used as is or after editing as a clipping polygon in the [grid2d-3d] plugin. To edit the polygon in the 3D scene a 3D horizon of suitable extent must be displayed eg such as a DataExtentHorizon.\nThe tool is accessed from the Tools submenu of the Polygon tree context menu.\nExample of polygons created by tool\nInput Convex Hull tool input dialog\nThe tool dialog allows:\nSpecifying a constant Z value for the output polygon Selecting 2D and 3D seismic data or horizon interpretation to use as the polygon scope Specifying the name and display settings of the generated polygon. Constant Z Polyline Description This tool allows a series of points to be digitized in the 3D scene that can subsequently be used as additional surface control in the Grid 2D3D plugin.\nThe tool is accessed from the Tools submenu of the Polygon tree context menu. After adding a contour polyline remember to use the Right-Click | Save menu to commit the item.\nExample of some polylines drawn in a gap in the seismic coverage to guide subsequent gridding\n###Input\nConstant Z polyline tool input dialog\nFault Surface-3D Horizon Intersection Polyline Description This tool generates 3D poly-lines at the intersection of 3D horizons and fault surfaces. The intersection line is made by joining the intersections of the fault sticks with the horizon. A potential use of the tool is to grid a 3D horizon without any fault constraint, use the tool to gererate fault lines at the intersection of the 3D horizon and existing fault surfaces and then regrid the horizon using the new fault lines as a fault constraint. (Note the performance of this tool was substantially improved as of version 6.4.13)\nThe tool is accessed from the Tools submenu of the Polygon tree context menu.\nExample of polylines generated by this tool\nInput The UI specifies a poly-line name prefix to which the fault surface names will be appended.\nFault polyline generation tool input dialog\nData Extent Horizon Description Generate a constant Z value 3D horizon which covers the extent of 2D and 3D seismic data in an [opendtect] survey/project. The user has full control over the 2D and 3D data included. With the horizon displayed it is possible to draw fault polygons and grid clipping polygons that extend beyond the 3D survey definition. These can then be used in the [grid2d-3d] plugin to produce structure maps that extend across both 2D and 3D seismic interpretation in an [opendtect] survey/project.\nThe tool is accessed either from the “Processing|Create Horizon Output” main menu item or from the New submenu of the 3D Horizon tree context menu.\nExample of an horizon created by the plugin and various polygons drawn across the extent of the 2D and 3D seismic data in the project\nInput Data Extent Horizon tool input dialog\nThe tool dialog allows:\nSpecifying a constant Z value for the output horizon Selecting 2D and 3D seismic data that the horizon should cover Specifying the name of the generated horizon ","categories":["docs"],"description":"miscellaneous tools for OpendTect","excerpt":"miscellaneous tools for OpendTect","ref":"/WMPlugin-Docs/docs/plugins/wmtools/","tags":["WMTools","convex-hull","polyline","data-extent-horizon"],"title":"WMTools"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/docs/","tags":"","title":"Documentation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/tags/avopolarattrib/","tags":"","title":"AVOPolarAttrib"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/tags/efdattrib/","tags":"","title":"EFDAttrib"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/tags/externalattrib/","tags":"","title":"ExternalAttrib"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/tags/mistie/","tags":"","title":"Mistie"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/tags/python/","tags":"","title":"python"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/categories/release/","tags":"","title":"release"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/tags/","tags":"","title":"Tags"},{"body":"Announcing the release of version 6.6.10 of the WMPlugins a suite of opensource plugins that extend the opensource seismic interpretation system OpendTect. This release is built against OpendTect 6.6.7.\nThe release includes the following changes:\nAVOPolarAttrib Nans and Infs generated during the calculation of the attributes are converted to OpendTect Undefined, fixing a crash.\nEFDAttrib Two new attributes have been added that use Empirical Fourier Decomposition (Zhou etal(2019). The two attributes generate the mode decomposition and a spectral decomposition from the mode decomposition. These provide a signal analysis akin to Empirical Mode Decomposition. A subsequent post will look at these new attributes in detail and compare the new spectral decomposition option with the existing WMPlugin tools such as Recursive spectral decomposition and Spectral decomposition by local attribute.\nEmpirical Fourier Mode/Spectral Decomposition\nExternal attributes This release fixes a crash when loading an attribute set with either a non-existent python interpreter or attribute script and now reports syntax errors from the script when building the UI via a UI message box. Previously these errors were recorded in the OpendTect log file but the user did not get any direct feedback that an issue was encountered.\nImproved error reporting in the External Attribute plugin\nAlso fixed various issues with the included wmpy scripts to make them compatible with the versions of Python modules installed by the OpendTect installation manager.\nMistie Analysis Fixed a crash when estimating misties for depth surveys.\nwmodpy - OpendTect Python Bindings A decision was made to discontinue development of these within WMPlugins in favour of moving the code into the dGB managed OpendTect open source code repository system on Github. The new repository will be called “odpybind”.\n","categories":["release"],"description":"","excerpt":"Announcing the release of version 6.6.10 of the WMPlugins a suite of …","ref":"/WMPlugin-Docs/blog/2022/04/15/wmplugins-6.6.10-release/","tags":["AVOPolarAttrib","EFDAttrib","ExternalAttrib","Mistie","Python"],"title":"WMPlugins 6.6.10 Release"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/categories/article/","tags":"","title":"article"},{"body":"The 6.6.8 release of the wmPlugins includes a Python module, wmodpy, to access OpendTect survey and well information. Unlike OpendTect’s existing odpy module, wmodpy is a direct binding to the OpendTect C++ code.\nThe odpy module, by comparison, uses a command line application to interact with the OpendTect data structures and transfers data to Python as an ascii data stream. This incurs the overhead of starting this application and the writing and reading of an ascii data stream on every request. The wmodpy module, however, as a direct binding to the OpendTect C++ code allows Python to directly read from the in-memory representation of the data. Data access with the wmodpy module should be much faster.\nAccessing the wmodpy Module Installing my wmPlugins using the OpendTect installation manager will install the wmodpy Python module into the bin/win64/Release or bin/lux64/Release subfolder of the OpendTect software. You also need a Python environment with at least Numpy but additional output options exist if the environment also has Pandas. The OpendTect machine learning Python environments already include Numpy and Pandas. For map display you may want to add a module like Folium to your working environment.\nThe folder containing the bindings library must be on the PYTHONPATH to be found by an import statement in a Python script or notebook. You can either use the OpendTect Python Settings dialog (accessible from the Utilities| Installation|Python Settings application menu) and add the location of the OpendTect executable files to the Custom Module Path or modify the PYTHONPATH within the script.\nOpendTect Python Settings Dialog\nThe OpendTect Python Settings dialog also allows you to select a custom Python environment to use and also add an icon to the OpendTect toolbar to start your chosen IDE and console (only OpendTect 6.6.4) with the specified custom Python environment activated and the Custom Module Paths added to the PYTHONPATH.\nThe alternative script/notebook based solution requires something like the following at the top of the script:\nimport sys sys.path.insert(0, 'C:/Program Files/OpendTect/6.6.0/bin/win64/Release') import wmodpy General Survey Information The module includes a number of functions for getting information about OpendTect surveys/projects. These all require an OpendTect data root folder for context.\nCommand Description get_surveys(data_root:str) Return list of survey names in the given data_root get_survey_info(data_root:str, surveys:list) Return a Python dictionary with basic information for the surveys in the given list or all surveys if no list is given get_survey_info_df(data_root:str, surveys:list) Return a Pandas Dataframe with basic information for the surveys in the given list or all surveys if no list is given get_survey_features(data_root:str, surveys:list) Return a GeoJSON Feature Collection with basic information for the surveys in the given list or all surveys if no list is given Some examples:\nwmodpy.get_surveys(\"/mnt/Data/seismic/CooperBasin/ODData\") ['13CP06_Dundinna', 'Cooper2D'] wmodpy.get_surveys_df(\"/mnt/Data/seismic/ODData\", [\"F3_Demo_2020\", \"Penobscot\"]) Displaying OpendTect survey info in a Pandas Dataframe\nDisplaying OpendTect survey locations on a Folium web map\nSurvey Class The module adds a Survey Python class. Create a Survey object for a particular data_root and survey_name combination like this:\nf3demo = wmodpy.Survey(\"/mnt/Data/seismic/ODData\", \"F3_Demo_2020\") The Survey object can then be used to get information about the survey:\nf3demo.info() {'Name': ['F3 Demo 2020'], 'Type': ['2D3D'], 'crs': ['EPSG:23031']} To get a full list of methods provided by the Survey class use :\nhelp(wmodpy.Survey) The other classes added by the module generally require a Survey object for context.\nWells Class The Wells Python class provides methods to access OpendTect well data within a survey. Creating a Wells object requires a Survey object for context:\nf3demo_wells = wmodpy.Wells(f3demo) Methods are provided to:\nList the names of all wells in the project/survey List summary information on all wells List well log information in a well List well log data List markers in a well List a well track There is also a “features” method that returns a GeoJSON Feature Collection. To get a full list of methods provided by the Wells class use:\nhelp(wmodpy.Wells) You will notice that in many cases the same data can be obtained as either a Python dictionary or as a Pandas Dataframe. Methods returning a Pandas Dataframe have a suffix of “_df”.\nf3demo_wells.info() {'Name': ['F02-1', 'F03-2', 'F03-4', 'F06-1'], 'UWID': ['', '', '', ''], 'State': ['', '', '', ''], 'County': ['', '', '', ''], 'WellType': ['none', 'none', 'none', 'none'], 'X': [606554.0, 619101.0, 623255.98, 607903.0], 'Y': [6080126.0, 6089491.0, 6082586.87, 6077213.0], 'ReplacementVelocity': [2000.0, 2000.0, 2000.0, 2000.0], 'GroundElevation': [1.0000000150474662e+30, 1.0000000150474662e+30, 1.0000000150474662e+30, 1.0000000150474662e+30]} f3demo_wells.info_df() Displaying OpendTect well info in a Pandas Dataframe\nGetting well log data in a Pandas Dataframe is as easy as:\nf3demo_wells.log_data_df('F02-1',['Vp','Sonic'],0.15, wmodpy.Wells.SampleMode.Upscale) Displaying OpendTect well logs in a Pandas Dataframe\n","categories":["release","article"],"description":"","excerpt":"The 6.6.8 release of the wmPlugins includes a Python module, wmodpy, …","ref":"/WMPlugin-Docs/blog/2021/06/29/first-release-of-python-bindings-to-opendtect/","tags":["python"],"title":"First Release of Python Bindings to OpendTect"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/tags/inversion/","tags":"","title":"inversion"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/tags/pylops/","tags":"","title":"pylops"},{"body":"The 6.6.8 release of the wmPlugins includes a number of new Python external attribute scripts that use the PyLops linear operator library for seismic modelling and inversion. These scripts also demonstrate the use of the new user interface parameter elements introduced in the 6.6.8 release of the External Attribute plugin.\nAssuming wmPlugins is installed using the OpendTect Installation Manager, the scripts will be in the bin/python/wmpy/PyLops folder of the OpendTect software folder and include:\nPyLops/ex_poststack_inversion.py PyLops/ex_poststack_relative_inversion.py PyLops/ex_poststack_modelling.py PyLops/ex_prestack_modelling.py PyLops/ex_make_1d_seismic.py Using these scripts requires a Python environment with the PyLops python module and it’s dependencies installed. The section Installing PyLops describes the PyLops installation process. Note that a PyLops python environment includes all the modules required to run any attribute script included with the wmPlugins not just those in the PyLops folder.\nPost-Stack Seismic Inversion The PyLops/ex_poststack_inversion.py script uses the pylops.avo.poststack.PoststackInversion operator to do post-stack seismic inversion. The output is either the log Acoustic Impedance (AI) volume or the residual error.\nThe inputs required are volumes of the seismic to be inverted, a background log AI model and the seismic wavelet. Note that the polarity of the seismic wavelet must match the data.\nThe following figures show inversion input and output for a 1D model created by the PyLops/ex_make_1d_seismic.py script.\nImpedance Model (red) and Background Model (blue)\nImpedance Model (red) and Seismic Model (blue)\nImpedance Model (red) and Inverted Impedance (blue)\nThere is also a PyLops/ex_poststack_relative_inversion.py script that runs the inversion without a background model: Impedance Model (red) and Inverted Relative Impedance (blue)\nInverted Relative Impedance Example\nPrestack Modelling The PyLops/ex_prestack_modelling.py script uses the pylops.avo.avo.AVOLinearModelling operator to create a pre-stack angle volume from well data. The output is either an Aki-Richards or Fatti approximate reflectivity model filtered by a user specified wavelet.\nThe inputs required are 3 log data cubes with compressional sonic (DT in us/m), shear sonic (DTS in us/m) and density (RHOB in g/cc). These can be created from well log data using the “Create Log Cube” right mouse button context menu in the scene well tree or the “Processing|Create Seismic Output|From Well Logs” main menu. Also needed is a wavelet with the appropriate polarity for the data being modelled.\nThe generated synthetics can be displayed in the 3D window and compared with real angle stack data through the well location. Installing PyLops The PyLops Python package and it’s dependencies can be installed in an active conda environment using:\nconda install -c conda-forge pylops or\npip install pylops See the PyLops documentation for more information.\n","categories":["article"],"description":"","excerpt":"The 6.6.8 release of the wmPlugins includes a number of new Python …","ref":"/WMPlugin-Docs/blog/2021/06/26/seismic-modelling-and-inversion-using-pylops-based-python-external-attributes/","tags":["python","ExternalAttrib","pylops","inversion"],"title":"Seismic Modelling and Inversion using PyLops based Python External Attributes"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/categories/howto/","tags":"","title":"howto"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/tags/jupyter/","tags":"","title":"Jupyter"},{"body":"In this post I show you how to set up and use the free Visual Studio Code editor with OpendTect for interactive computation using Python Jupyter Notebooks. If you have installed the OpendTect Machine Learning Python environments you can create and edit Python Jupyter Notebooks using Visual Studio Code without modifying the installed environment provided VS Code is run from within OpendTect. The approach described in this post might assist users in strictly controlled IT settings get up and running using Jupyter Notebooks.\nInstalling Visual Studio Code VS Code should be available in the package repositories of all the major Linux distributions so just install it like you would any other application. Alternatively the VS Code Download page provides a tar.gz download for 64 bit Linux that can be copied into a users home folder for those situations where IT settings prevent a global software install.\nFor Windows go to the VS Code Download page and download the package of choice. There are 3 options:\nUser installer (software ends up in “C:\\users{username}\\AppData\\Local\\Programs\\Microsoft VS Code”, recommended way) System installer (software ends up in “C:\\Program Files”, requires Administrator privileges) .zip (you unpack the zip file anywhere even on a usb stick) Once installed, start it up (find the executable file call “code” in the application folder and run it from a console or click/double click on it from your file manager/explorer) and add the following extensions:\nPython extension for Visual Studio Code Jupyter Extension for Visual Studio Code Selecting Visual Studio Code as the OpendTect IDE Open the OpendTect Python Settings dialog (Utilities|Installation|Python Settings menu) OpendTect Python Settings Dialog\nFor the Python IDE select “Other” Use the File Selector to locate the executable file called “code” in the VS Code application folder Use the icon button to set the location of an icon for the application (normally in the resources/app/resources/(linux|win32) subfolder of the VS Code installation) Set a tooltip message, eg “VS Code” Press OK This will add an icon to the OpendTect toolbar and a new menu item to the “Utilities|User Commands” menu to start Visual Studio Code. Starting VS Code from within OpendTect ensures environment settings are compatible with the Python environment selected in the OpendTect Python Settings dialog.\nOpendTect After Adding VS Code\nClick the icon or use the “Utilities|User Commands” menu to start a Visual Studio Code instance.\nUsing Visual Studio Code With these steps completed it should be possible to start VS Code from OpendTect and open a new blank Jupyter notebook by running the Jupyter: Create Blank New Jupyter Notebook command from the VS Code Command Palette (Ctrl+Shift+P). This notebook will use the Python environment selected in the OpendTect Python Settings dialog. New Jupyter Notebook in VS Code\nTo find out more about using VS Code with Jupyter Notebooks check out the documentation:\nWorking with Jupyter Notebooks in Visual Studio Code Working with the Python Interactive window Exploring OpendTect Well Data in a VS Code Notebook\n","categories":["howto"],"description":"","excerpt":"In this post I show you how to set up and use the free Visual Studio …","ref":"/WMPlugin-Docs/blog/2021/06/05/using-visual-studio-code-with-opendtect/","tags":["python","VSCode","Jupyter"],"title":"Using Visual Studio Code with OpendTect"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/tags/vscode/","tags":"","title":"VSCode"},{"body":"Summary The potential to offload calculations to the graphics processing unit (GPU) on modern graphics cards is a trending topic. I was curious if this could also apply to seismic attribute calculation in OpendTect, so I implemented AVO polarization angle estimation as described by Mahob and Castagna (2003) using 3 different approaches:\nusing loops for all the linear algebra and OpendTect’s multi dimensional arrays (ArrayNDImpl) which I refer to as the Normal method; using the Eigen linear algebra C++ template library (the Eigen method) and using the ArrayFire linear algebra library for GPU’s (the ArrayFire method). The first 2 options only use the central processing unit (CPU). ArrayFire also supports CPU calculation but for this review only the GPU capabilities were evaluated. In all cases benchmarks were run with and without OpendTect’s multi-threaded processing support. Results are summarised in the following graph showing performance relative to the single threaded Normal method for a range of input data stepouts (higher performance numbers are better):\nThe primary observations are:\nBoth Eigen and ArrayFire can be used relatively easily within OpendTect 6.4 attribute plugins; OpendTect’s multi-threaded processing support offers the most performance enhancement so developing the algorithm to enable multi-threading should be the first aim; The Eigen library offers a noticeable performance enhancement of up to 20% (relative to explicit coding of the linear algebra) for single trace and low-medium stepout attributes. Algorithms can also be expressed in fewer lines of code that is simpler to understand. The ArrayFire library makes it relatively easy to access the GPU compute power on the graphics card but the overhead of transferring data from the CPU to the GPU is significant and can result in poorer performance in some cases. The scale and/or compute complexity of the calculations will determine if there is any benefit to GPU calculations. In general this will require benchmarking of the particular algorithm. The code used in this evaluation is available in my OpendTect-Plugins GitHub repository. Related details are described below.\nHardware Configuration Intel Core i5-3470 CPU @ 3.2Ghz 16GiB RAM GeForce GTX1050 Ti with 4GiB RAM Sofware Configuration Operating System:\nx86_64 GNU/Linux 4.2.0 Software:\nOpendTect 6.4.2 gcc 8.2.1 Eigen 3.3.7 ArrayFire 3.6.1 CUDA Toolkit 10, Driver: 415.25, CUDA Compute 6.1 Implementation The attribute plugin source code is in my OpendTect-Plugins GitHub repository in the:\nNoRelease/AVOPolar and NoRelease/uiAVOPolar folders. The plugin user interface is shown in the following image:\nImports Section\nA comparison of the compute code for each method is shown below. All code assumes multi-trace intercept and gradient data in the 2D arrays with dimensions (sz x ntraces) named A and B respectively. Comparison of the code reveals both Eigen and ArrayFire allow significantly more concise and clearer coding of linear algebra calculations.\nNormal Method Array1DImpl\u003cdouble\u003e A2(sz); Array1DImpl\u003cdouble\u003e B2(sz); Array1DImpl\u003cdouble\u003e AB(sz); for (int idx=0; idx\u003csz; idx++) { double A2v = 0.0; double B2v = 0.0; double ABv = 0.0; for (int trcidx=0; trcidx\u003cntraces; trcidx++) { A2v += (double) A.get(trcidx, idx) * (double) A.get(trcidx, idx); ABv += A.get(trcidx, idx) * B.get(trcidx, idx); B2v += B.get(trcidx, idx) * B.get(trcidx, idx); } A2.set(idx, A2v); B2.set(idx, B2v); AB.set(idx, ABv); } Array1DImpl\u003cdouble\u003e A2win(sz); windowedOps::sum( A2, sampgateBG_.width(), A2win ); Array1DImpl\u003cdouble\u003e B2win(sz); windowedOps::sum( B2, sampgateBG_.width(), B2win ); Array1DImpl\u003cdouble\u003e ABwin(sz); windowedOps::sum( AB, sampgateBG_.width(), ABwin ); for (int idx=0; idx\u003csz; idx++) { double ABv = ABwin.get(idx); double A2mB2 = A2win.get(idx) - B2win.get(idx); double d = sqrt(4.0*ABv*ABv + A2mB2*A2mB2); result.set(idx, atan2(2.0*ABv, A2mB2+d)/M_PI*180.0); } for (int idx=0; idx\u003cnrsamples; idx++) setOutputValue(output, 0, idx, z0, result(idx-sampgateBG_.start)); Eigen Method Eigen::ArrayXd A2 = A.square().rowwise().sum(); Eigen::ArrayXd B2 = B.square().rowwise().sum(); Eigen::ArrayXd AB = (A*B).rowwise().sum(); Eigen::ArrayXd A2win(sz); windowedOpsEigen::sum( A2, sampgateBG_.width(), A2win ); Eigen::ArrayXd B2win(sz); windowedOpsEigen::sum( B2, sampgateBG_.width(), B2win ); Eigen::ArrayXd ABwin(sz); windowedOpsEigen::sum( AB, sampgateBG_.width(), ABwin ); Eigen::ArrayXd A2mB2 = A2win - B2win; Eigen::ArrayXd result = (2.0*ABwin/(A2mB2 + (4.0*ABwin.square() + A2mB2.square()).sqrt())).atan()/M_PI*180.0; for (int idx=0; idx\u003cnrsamples; idx++) setOutputValue(output, 0, idx, z0, result(idx-sampgateBG_.start)); ArrayFire Method af::array A2 = af::sum(A*A,1); af::array B2 = af::sum(B*B,1); af::array AB = af::sum(A*B,1); af::array kernel = af::constant( (double) 1.0, sampgateBG_.width(),1); af::array A2win = af::convolve1(A2, kernel); af::array B2win = af::convolve1(B2, kernel); af::array ABwin = af::convolve1(AB, kernel); af::array A2mB2 = A2win - B2win; af::array result = af::atan2(2.0*ABwin, A2mB2 + af::sqrt(4.0*ABwin*ABwin + A2mB2*A2mB2))/M_PI*180.0; double* resbuf = result.host\u003cdouble\u003e(); for (int idx=0; idx\u003cnrsamples; idx++) setOutputValue(output, 0, idx, z0, resbuf[idx-sampgateBG_.start]); af::freeHost(resbuf); Method Attribute definitions were created for a range of stepouts, using a fixed window length, with/without multi-threading for each of the methods (Normal, Eigen and ArrayFire) Input data volumes with ~100,000 traces were preloaded into RAM The attributes were run using the OpendTect Processing|Create Seismic Output|Attributes|Single Attribute| menu item. Elapsed time to evaluate the attribute were computed from the start and finish times reported in the Progress Viewer Repeat runs conducted to verify consistency of the results. ","categories":["article"],"description":"","excerpt":"Summary The potential to offload calculations to the graphics …","ref":"/WMPlugin-Docs/blog/2019/01/07/gpu-vs-cpu-benchmarks-for-opendtect-attribute-plugins/","tags":["AVOPolarAttrib"],"title":"GPU vs CPU Benchmarks for OpendTect Attribute Plugins"},{"body":"Introduction Because the Python [../plugins/ExternalAttrib] script is running in a process started by the OpendTect application most standard methods to examine the script as it runs, eg using the standard Python debugger pdb, are not available.\nA solution is to use the Web-PDB Python module which allows the Python script to be debugged remotely in a web-browser.\nWeb-PDB Installation Web-PDB is not included by default in most Python installations but it can be easily added using pip:\npip install web-pdb\nAdding Web-PDB to a Script Adding Web-PDB to a script is just a matter of importing the module as shown on line 5 and adding a call to web_pdb.set_trace() as shown on line 19. The web_pdb.set_trace() call acts like a breakpoint and can be inserted as many times as required.\nFor simplicity it is best to disable multi-threaded processing (add a Parallel: False line to the xa.params object) while debugging.\nDebugging with Web-PDB After adding Web-PDB to the [../plugins/ExternalAttrib] script it will run to the first breakpoint where execution will be suspended and a web-UI opened at the default port 5555. Pointing a web browser at http://\u003cyour machine hostname or IP\u003e:5555, eg http://127.0.0.1:5555, should show an interface for debugging as above.\nThe buttons provide control on the script execution, hover the mouse pointer over them to see tooltips for each. More complex pdb commands can be inserted in the entry at the bottom of the screen. Click the ? button for a list of useful pdb commands.\nWeb-PDB and Multi-threaded Processing It is possible to use Web-PDB with a script that has multi-threaded processing enabled by replacing the initial web_pdb.set_trace call with:\nweb_pdb.set_trace( port=-1 )\nThis will cause each Python process to select a random port between 32768 and 65536. Operating System specific commands can then be used to determine the ports opened, eg:\nOn Linux: ss -lntu in a console window.\nOn Windows: netstat -an in a command window.\nA web-UI will need to be opened for each port and each process will need to be stepped through all breakpoints for attribute execution to progress.\n","categories":["howto"],"description":"","excerpt":"Introduction Because the Python [../plugins/ExternalAttrib] script is …","ref":"/WMPlugin-Docs/blog/2018/06/08/python-external-attribute-tips-tricks-debugging/","tags":["python","ExternalAttrib"],"title":"Python External Attribute Tips \u0026 Tricks - Debugging"},{"body":"Introduction It is possible to write information to the OpendTect logfile from inside a Python [../plugins/ExternalAttrib] script.\nThe global variable xa.logH (assuming the extattrib module has been imported using import extattrib as xa) is a Python logger object.\nAn Example On line 18 the Python logger is modified by adjusting the severity level of messages that will appear in the log file. By default only CRITICAL, ERROR and WARNING messages will be written.\nOn line 22 a message is written to the logfile showing the full path to the Python interpreter executing the script. As this line is in the Compute Loop Initialisation section it is only written at each invocation of the script.\nOn line 32 a message is written that identifies the location, minimum and maximum of the trace being processed. As this line is in the Compute Loop a message is output for every trace processed.\nThe Result ","categories":["howto"],"description":"","excerpt":"Introduction It is possible to write information to the OpendTect …","ref":"/WMPlugin-Docs/blog/2018/06/06/python-external-attribute-tips-tricks-logging/","tags":["python","ExternalAttrib"],"title":"Python External Attribute Tips \u0026 Tricks - Logging"},{"body":"Introduction This article will review the structure of a simple Python [../plugins/ExternalAttrib] script, ex_dip.py, which converts inline and crossline dip to true dip and dip azimuth. It is an example of multi attribute, single trace input and output. Some basic understanding of Python and Numpy is assumed.\nEvery Python attribute script has 5 sections.\nThe Imports Imports Section\nThis is where external modules/libraries required by the script are loaded. At a minimum the script must load:\nthe Python sys and os modules the Numpy module (the fundamental package for scientific computing with Python) the external attribute module (extattrib.py) Generally sys, os and Numpy will be part of the Python installation. The extattrib module is part of the [wmscripts] package and its location is unknown to the Python installation unless we help out. The sys.path.insert call on line 11 provides this help by extending the default search path for Python modules to include the parent folder of the folder containing the script. This reflects the folder structure of the [wmscripts] package, so if you develop scripts outside this structure then you will need to change line 11 appropriately to append the location of extattrib.py to the module search path.\nOf course if your script requires other Python modules (eg SciPy, Numba) then add the appropriate import statements in this section.\nThe Parameters UI Parameters\nThe xa.params global variable must be assigned a JSON object string describing the input parameters for the script. This JSON string is used by the plugin to build an input dialog box. This attribute is very simple specifying just 2 input volumes and 2 output volumes and a url for documentation. The plugin dynamically builds the following input dialog for this script:\nUI Appearance\nA variety of other input elements can be specified to build more complex input dialogs. See the [../plugins/ExternalAttrib#JSON Parameter String] section of the plugin documentation for full details or look at other scripts to see what is possible.\nThe Compute Loop Initialisation Compute Initialisation\nThe doCompute function is where the attribute calculation occurs. The function is divided into 2 parts some initialisation and the “while True:” loop, discussed in the next section, where the calculations actually take place. Any code in this initialisation section will be executed just once when the attribute script is run and is a good place to calculate constants for use in the Compute Loop.\nThis particular script shows how information stored in the [../plugins/ExternalAttrib#SeismicInfo Block] can be used to calculate some constants purely as an example. This attribute is so simple that no initialisation is actually required.\nThe Compute Loop Compute Loop\nThis is where the attribute calculation takes place. The xa.doInput() and xa.doOutput() function calls control the input and output of seismic trace data between the script and OpendTect. Generally these should be the first and last statements within the compute loop.\nWithin the compute loop, some information about the current trace data such as the number of samples and the inline and crossline location are provided in the [../plugins/ExternalAttrib#TraceInfo Block]. These can be accessed using constructs like xa.TI[’nrsamp’]. This information is not required for this particular script.\nThe global Numpy array xa.Input contains the input trace data. xa.Input[’name of input attribute’] returns a Numpy array with the trace data for the current compute location. The shape of this Numpy array depends on the traces stepouts required by the attribute. As this particular script uses just single trace input (inline and crossline step out of 0) the Numpy array has a shape of (1,1,xa.TI[’nrsamp’]). In the more general case of a multi-trace attribute the Numpy array shape would be (xa.SI[’nrinl’], xa.SI[’nrcrl’], xa.TI[’nrsamp’]) and the input trace at the current location would be at the centre of the array, ie at index [xa.SI[’nrinl’]//2, xa.SI[’nrcrl’]//2,…].\nAttribute ouput must be put into the xa.Output global Numpy array before the xa.doOutput() function call. Each element (eg xa.Output[’name of output attribute’]) of the output array must have a shape of (1,1,xa.TI[’nrsamp’])\nThe Postamble Wrapping up\nThis section is just boilerplate code that apprears in every attribute script which should never be changed.\n","categories":["howto"],"description":"","excerpt":"Introduction This article will review the structure of a simple Python …","ref":"/WMPlugin-Docs/blog/2018/06/04/anatomy-of-a-python-external-attribute/","tags":["python","ExternalAttrib"],"title":"Anatomy of a Python External Attribute"},{"body":" About WMPlugins Open source OpendTect plugins and external attribute scripts by WM Seismic Solutions. ","categories":"","description":"","excerpt":" About WMPlugins Open source OpendTect plugins and external attribute …","ref":"/WMPlugin-Docs/about/","tags":"","title":"About WMPlugins"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/tags/attribute/","tags":"","title":"attribute"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/tags/avo/","tags":"","title":"avo"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/tags/avoattrib/","tags":"","title":"AVOAttrib"},{"body":"This is the blog section. It has two categories: News and Releases.\nFiles in these directories will be listed in reverse chronological order.\n","categories":"","description":"","excerpt":"This is the blog section. It has two categories: News and Releases. …","ref":"/WMPlugin-Docs/blog/","tags":"","title":"WMPlugins Blog"},{"body":" ","categories":"","description":"","excerpt":" ","ref":"/WMPlugin-Docs/community/","tags":"","title":"Community"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/tags/convex-hull/","tags":"","title":"convex-hull"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/tags/data-extent-horizon/","tags":"","title":"data-extent-horizon"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/tags/dipaz/","tags":"","title":"dipaz"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/categories/docs/","tags":"","title":"docs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/tags/filter/","tags":"","title":"filter"},{"body":"FAQ Can’t find an answer here - then submit an issue on Github.\nGeopackage Export plugin isn’t loading The GeopackageExport plugin requires access to external files (DLL’s on Windows and lib*.so on Linux). These should have been included in the binary package for the plugins.\nOn Windows the folder containing the plugin and support DLL’s must be added to the PATH environment variable. Do this by either:\nEditing/Adding the PATH environment variable for the System or User (Control Panel\u003eSystem and Security\u003eSystem - Advanced system settings - Environment Variables) Adding a line like “@set PATH=%HOMEPATH%.od\\bin\\win64\\Release;%PATH%” (adjust “%HOMEPATH%.od\\bin\\win64\\Release” to reflect your installation) to the bat script used to start OpendTect Plugins not loading Try manually loading the plugin.\nCheck the OpendTect log file for error messages and see if there is already a solution outlined elsewhere in this page.\nPer-user Installation and Multiple OpendTect versions The OpendTect-6.4-plugins won’t work in OpendTect 6.2 and the OpendTect-6.2-plugins won’t work in OpendTect 6.4. Here is a way to use the plugins with mutliple versions of OpendTect.\nUse the OD_USER_PLUGIN_DIR environment variable For Windows 1.Create ODPlugins\\6.4.0 and ODPlugins\\6.2.0 folders in the C:\\Users%username% folder\n2.Install the OpendTect-6.2-plugins in the 6.2.0 folder and the OpendTect-6.4-plugins in the 6.4.0 folder as per the [installation] instructions.\n3.Create a “bat” file to start each version of OpendTect that sets the OD_USER_PLUGIN_DIR environment variable to the appropriate folder before starting OpendTect. Here is what odt_6_4.bat might look like:\n@set OD_USER_PLUGIN_DIR=%HOMEPATH%\\ODPlugins\\6.4.0 start \"\" \"C:\\Program Files\\OpendTect\\6.4.0\\bin\\win64\\Release\\od_start_dtect.exe\" For Linux 1.Create ODPlugins\\6.4.0 and ODPlugins\\6.2.0 folders in the users home directory\nmkdir ~/ODPlugins mkdir ~/ODPlugins/6.4.0 mkdir ~/ODPlugins/6.2.0 2.Install the OpendTect-6.4-plugins in the users 6.4.0 folder and the OpendTect-6.2-plugins in the 6.2.0 folder as per the [installation] instructions.\n3.Create executable shell scripts to start each version of OpendTect that sets the OD_USER_PLUGIN_DIR to the appropriate folder before starting OpendTect. Here is what odt_6_4.csh might look like:\n#!/bin/csh -f setenv OD_USER_PLUGIN_DIR \"$HOME/ODPlugins/6.4.0\" /path to OpendTect 6.4/start_dtect libstdc++.so.6: version ‘GLIBCXX_3.4.??’ not found This happens when the plugin is built with a gcc version different to the version used to build OpendTect. Solutions are:\n(Easy and seems to work ok but could break something) Rename the libstdc++.so.6 file in the OpendTect installation bin/lux64 folder to say old_libstdc++.so.6 and restart OpendTect.\n(Hard) Install the same version of gcc that OpendTect was built with and rebuild the plugin.\n(Hardest) Build OpendTect from source using your installed gcc.\n","categories":"","description":"","excerpt":"FAQ Can’t find an answer here - then submit an issue on Github. …","ref":"/WMPlugin-Docs/docs/faq/faq/","tags":"","title":"Frequently Asked Questions"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/tags/geopackage/","tags":"","title":"Geopackage"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/tags/geotif/","tags":"","title":"geotif"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/tags/gradientattrib/","tags":"","title":"GradientAttrib"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/tags/gridding/","tags":"","title":"gridding"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/tags/mlvfilterattrib/","tags":"","title":"MLVFilterAttrib"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/tags/modelling/","tags":"","title":"modelling"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/tags/polyline/","tags":"","title":"polyline"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/tags/qgis/","tags":"","title":"qgis"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/search/","tags":"","title":"Search Results"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/tags/spectral-decomposition/","tags":"","title":"spectral-decomposition"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/tags/time-frequency/","tags":"","title":"time-frequency\""},{"body":" WMPlugins - free, open source plugins for OpendTect Learn More Install This is the documentation site for various plugins and external attribute scripts I have developed for the open source seismic interpretation system OpendTect.\nThe plugins are made available under the terms of the GNU General Public License Version 3.\n","categories":"","description":"","excerpt":" WMPlugins - free, open source plugins for OpendTect Learn More …","ref":"/WMPlugin-Docs/","tags":"","title":"WMPlugins-Docs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/WMPlugin-Docs/tags/wmtools/","tags":"","title":"WMTools"}]